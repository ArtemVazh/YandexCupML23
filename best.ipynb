{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "340aceb6-4360-49ac-8381-bb42055d6146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.remove('/home/jovyan/.imgenv-lm-poly-0/lib/python3.7/site-packages')\n",
    "os.environ['PYTHONPATH'] = '/home/user/conda/envs/ya/lib/python3.10/site-packages'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32801790-6619-4141-b909-a70de5e00071",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_DEV = 0\n",
    "NUM_TAGS = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29f35090-eb3d-464b-b46f-de30c4d42d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "144deed2-ca15-41aa-9225-6edf01d1003e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "tags = [[int(i) for i in x.split(',')] for x in df_train.tags.values]\n",
    "dict_tags = {}\n",
    "for cls_tags in tags:\n",
    "    for c in cls_tags:\n",
    "        if c not in dict_tags.keys():\n",
    "            dict_tags[c] = Counter(cls_tags)\n",
    "        else:\n",
    "            dict_tags[c].update(Counter(cls_tags))\n",
    "            \n",
    "for tag in dict_tags.keys():\n",
    "    del dict_tags[tag][tag]\n",
    "    n = np.sum(list(dict_tags[tag].values()))\n",
    "    for t in dict_tags[tag].keys():\n",
    "        dict_tags[tag][t] = dict_tags[tag][t]/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46b7d4b-785e-4187-b2bc-6c620c7b8cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|â–‹         | 5077/76715 [00:09<02:10, 547.52it/s]"
     ]
    }
   ],
   "source": [
    "track_idx2embeds = {}\n",
    "for fn in tqdm(glob('track_embeddings/*')):\n",
    "    name = fn.split('/')[1].split('.')[0]\n",
    "    if name == \"track_embeddings\":\n",
    "        continue\n",
    "    track_idx = int(name)\n",
    "    embeds = np.load(fn)\n",
    "    track_idx2embeds[track_idx] = embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1b5cae-60ae-4584-a6bb-4f6b833929aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaggingDataset(Dataset):\n",
    "    def __init__(self, df, aug=0, testing=False):\n",
    "        self.df = df\n",
    "        self.testing = testing\n",
    "        self.aug = aug\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        track_idx = row.track\n",
    "        embeds = track_idx2embeds[track_idx]\n",
    "        if self.testing:\n",
    "            return track_idx, embeds\n",
    "        tags = [int(x) for x in row.tags.split(',')]\n",
    "        target = np.zeros(NUM_TAGS)\n",
    "        target[tags] = 1\n",
    "        \n",
    "        if np.random.choice([0, 1], p=[1 - self.aug, self.aug]):\n",
    "            s = np.random.uniform(0.0, 0.4)\n",
    "            e = np.random.uniform(s+0.1, 1)\n",
    "            s = int(s * embeds.shape[0])\n",
    "            e = int(e * embeds.shape[0])\n",
    "            embeds = embeds[s:e]\n",
    "        \n",
    "        return track_idx, embeds, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca9ac5cf-a481-4918-bbeb-ecf077c681ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TaggingDataset(df_train[:-1000], aug=0.6)\n",
    "val_dataset = TaggingDataset(df_train[-1000:])\n",
    "\n",
    "test_dataset = TaggingDataset(df_test, testing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31c659b7-ee4b-44da-a715-b7abced07279",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, emb_dim=768, mult=4, p=0.0):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(emb_dim, emb_dim * mult),\n",
    "            nn.Dropout(p),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(emb_dim * mult, emb_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "    \n",
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Sequential(\n",
    "            nn.Linear(embedding_size, embedding_size),\n",
    "            nn.LayerNorm(embedding_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embedding_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_logits = self.attn(x)\n",
    "        if mask is not None:\n",
    "            attn_logits[mask] = -float('inf')\n",
    "        attn_weights = torch.softmax(attn_logits, dim=1)\n",
    "        x = x * attn_weights\n",
    "        x = x.sum(dim=1)\n",
    "        return x\n",
    "    \n",
    "class Network(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes = NUM_TAGS,\n",
    "        input_dim = 768,\n",
    "        hidden_dim = 512\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.position_enc = nn.Embedding(128, input_dim, padding_idx=-1) \n",
    "        self.proj = FeedForward(input_dim)\n",
    "        self.bn = nn.BatchNorm1d(input_dim)\n",
    "        self.ln = nn.LayerNorm(input_dim)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=768, nhead=12, activation=\"gelu\", batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=6)\n",
    "        self.poooling = AttentionPooling(input_dim)\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "               \n",
    "    def forward(self, embeds):\n",
    "        embeds = self.proj(embeds)\n",
    "        src_key_padding_mask = (embeds.mean(-1) == -1)\n",
    "        embeds = self.ln(embeds)\n",
    "        x = self.transformer_encoder(embeds, src_key_padding_mask=src_key_padding_mask)\n",
    "        x = self.bn(self.poooling(x, mask=src_key_padding_mask))\n",
    "        outs = self.fc(x)\n",
    "        return outs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c70bf034-7966-4f44-9f2e-dcaf0f8a8184",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, scheduler, print_loss=True, iteration_step=100, epoch=0):\n",
    "    model.train()\n",
    "    running_loss = None\n",
    "    alpha = 0.8\n",
    "    iters = len(loader)\n",
    "    for iteration,data in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        track_idxs, embeds, target = data\n",
    "        embeds = [x.to(CUDA_DEV) for x in embeds]\n",
    "        embeds = pad_sequence(embeds, padding_value=-1, batch_first=True)[:, :64, :]\n",
    "        target = target.to(CUDA_DEV)\n",
    "        pred_logits = model(embeds)\n",
    "        pred_probs = torch.sigmoid(pred_logits)\n",
    "        ce_loss = criterion(pred_logits, target)\n",
    "            \n",
    "        ce_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        scheduler.step(epoch + iteration / iters)\n",
    "        \n",
    "        if running_loss is None:\n",
    "            running_loss = ce_loss.item()\n",
    "        else:\n",
    "            running_loss = alpha * running_loss + (1 - alpha) * ce_loss.item()\n",
    "        if (iteration % iteration_step == 0) and print_loss:\n",
    "            print('   {} batch {} running loss {} loss {}'.format(\n",
    "                datetime.now(), iteration + 1, running_loss, ce_loss.item()\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e624b0b-8daf-4702-a5de-c667fcd53121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, loader):\n",
    "    model.eval()\n",
    "    track_idxs = []\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            track_idx, embeds = data\n",
    "            embeds = [x.to(CUDA_DEV) for x in embeds]\n",
    "            embeds = pad_sequence(embeds, padding_value=-1, batch_first=True)[:, :64, :]\n",
    "            pred_logits = model(embeds)\n",
    "            pred_probs = torch.sigmoid(pred_logits)\n",
    "            predictions.append(pred_probs.cpu().numpy())\n",
    "            track_idxs.append(track_idx.numpy())\n",
    "    predictions = np.vstack(predictions)\n",
    "    track_idxs = np.vstack(track_idxs).ravel()\n",
    "    return track_idxs, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37c01c5a-6d40-47d3-a535-d3349b41139e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def predict_train(model, loader):\n",
    "    model.eval()\n",
    "    track_idxs = []\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            track_idx, embeds, target = data\n",
    "            embeds = [x.to(CUDA_DEV) for x in embeds]\n",
    "            embeds = pad_sequence(embeds, padding_value=-1, batch_first=True)[:, :64, :]\n",
    "            pred_logits = model(embeds)\n",
    "            pred_probs = torch.sigmoid(pred_logits)\n",
    "            predictions.append(pred_probs.cpu().numpy())\n",
    "            track_idxs.append(track_idx.numpy())\n",
    "            targets.append(target.numpy())\n",
    "    predictions = np.vstack(predictions)\n",
    "    targets = np.vstack(targets)\n",
    "    track_idxs = np.vstack(track_idxs).ravel()\n",
    "    return track_idxs, predictions, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4decde5-005a-4820-804c-cf4d110c799e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(b):\n",
    "    track_idxs = torch.from_numpy(np.vstack([x[0] for x in b]))\n",
    "    embeds = [torch.from_numpy(x[1]) for x in b]\n",
    "    targets = np.vstack([x[2] for x in b])\n",
    "    targets = torch.from_numpy(targets)\n",
    "    return track_idxs, embeds, targets\n",
    "\n",
    "def collate_fn_test(b):\n",
    "    track_idxs = torch.from_numpy(np.vstack([x[0] for x in b]))\n",
    "    embeds = [torch.from_numpy(x[1]) for x in b]\n",
    "    return track_idxs, embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62124a85-3bcc-4eb8-a0d7-931c8685d1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=128, shuffle=False, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False, collate_fn=collate_fn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "448ff0ee-2a61-4761-8d54-8b555bca7a09",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/conda/envs/ya/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2023-10-28 14:50:17.055754 batch 1 running loss 0.7308424547369654 loss 0.7308424547369654\n",
      "   2023-10-28 14:51:31.536043 batch 701 running loss 0.6817006676839591 loss 0.6817837555747133\n",
      "epoch: 0, AP: 0.10535884323029973\n",
      "   2023-10-28 14:51:41.369855 batch 1 running loss 0.6796277644900348 loss 0.6796277644900348\n",
      "   2023-10-28 14:52:56.811810 batch 701 running loss 0.6450572991579687 loss 0.6453651685236517\n",
      "epoch: 1, AP: 0.14464805845653567\n",
      "   2023-10-28 14:53:06.674733 batch 1 running loss 0.6373535739577588 loss 0.6373535739577588\n",
      "   2023-10-28 14:54:22.696326 batch 701 running loss 0.5720440619285229 loss 0.5733344379245074\n",
      "epoch: 2, AP: 0.1708249582689514\n",
      "   2023-10-28 14:54:32.739213 batch 1 running loss 0.5634246307950161 loss 0.5634246307950161\n",
      "   2023-10-28 14:56:15.106554 batch 701 running loss 0.48979199537589546 loss 0.4899965929767496\n",
      "epoch: 3, AP: 0.17386765768934992\n",
      "   2023-10-28 14:56:34.186536 batch 1 running loss 0.483616359162266 loss 0.483616359162266\n",
      "   2023-10-28 14:59:00.259420 batch 701 running loss 0.4206567790714896 loss 0.4199206741878555\n",
      "epoch: 4, AP: 0.18857942641612757\n",
      "   2023-10-28 14:59:19.255863 batch 1 running loss 0.41534963523873564 loss 0.41534963523873564\n",
      "   2023-10-28 15:01:45.186217 batch 701 running loss 0.37258082549620675 loss 0.3718469471997419\n",
      "epoch: 5, AP: 0.2090675029773155\n",
      "   2023-10-28 15:02:04.205301 batch 1 running loss 0.3687179982016412 loss 0.3687179982016412\n",
      "   2023-10-28 15:04:30.689514 batch 701 running loss 0.34138406291718004 loss 0.3415008756188436\n",
      "epoch: 6, AP: 0.2028926563013862\n",
      "   2023-10-28 15:04:49.773447 batch 1 running loss 0.3392280648797562 loss 0.3392280648797562\n",
      "   2023-10-28 15:07:16.281089 batch 701 running loss 0.32498971986641195 loss 0.3255109359072723\n",
      "epoch: 7, AP: 0.21397033211240024\n",
      "   2023-10-28 15:07:35.252881 batch 1 running loss 0.3233347784028512 loss 0.3233347784028512\n",
      "   2023-10-28 15:10:01.495125 batch 701 running loss 0.3187108316779732 loss 0.3210197566401895\n",
      "epoch: 8, AP: 0.2183498949079019\n",
      "   2023-10-28 15:10:20.397231 batch 1 running loss 0.31572988545853764 loss 0.31572988545853764\n",
      "   2023-10-28 15:12:46.817405 batch 701 running loss 0.3160911251955465 loss 0.3168820969472188\n",
      "epoch: 9, AP: 0.21625525918917393\n",
      "   2023-10-28 15:13:05.796645 batch 1 running loss 0.31745395231985235 loss 0.31745395231985235\n",
      "   2023-10-28 15:15:32.024689 batch 701 running loss 0.2414961845755155 loss 0.2415044702421106\n",
      "epoch: 10, AP: 0.2015735317619326\n",
      "   2023-10-28 15:15:51.060336 batch 1 running loss 0.23307117925250564 loss 0.23307117925250564\n",
      "   2023-10-28 15:18:17.435378 batch 701 running loss 0.1793822291960807 loss 0.1814518875311023\n",
      "epoch: 11, AP: 0.21091839485359087\n",
      "   2023-10-28 15:18:36.472289 batch 1 running loss 0.1758401738608355 loss 0.1758401738608355\n",
      "   2023-10-28 15:21:02.721452 batch 701 running loss 0.13983366898658783 loss 0.14073160289249254\n",
      "epoch: 12, AP: 0.22682582029002213\n",
      "   2023-10-28 15:21:21.852235 batch 1 running loss 0.13521386692822512 loss 0.13521386692822512\n",
      "   2023-10-28 15:23:48.369911 batch 701 running loss 0.11542107950563837 loss 0.11583868103931363\n",
      "epoch: 13, AP: 0.23927088880855338\n",
      "   2023-10-28 15:24:07.374596 batch 1 running loss 0.11229090601126471 loss 0.11229090601126471\n",
      "   2023-10-28 15:26:33.863646 batch 701 running loss 0.09980868487463576 loss 0.10067938174815985\n",
      "epoch: 14, AP: 0.2466864420649518\n",
      "   2023-10-28 15:26:52.789093 batch 1 running loss 0.09771922636694153 loss 0.09771922636694153\n",
      "   2023-10-28 15:29:19.512546 batch 701 running loss 0.09138848778140048 loss 0.09245119485913289\n",
      "epoch: 15, AP: 0.2591945436112556\n",
      "   2023-10-28 15:29:38.630103 batch 1 running loss 0.08882520340580435 loss 0.08882520340580435\n",
      "   2023-10-28 15:32:05.018584 batch 701 running loss 0.0857680598206125 loss 0.0837993713363856\n",
      "epoch: 16, AP: 0.2639910576643468\n",
      "   2023-10-28 15:32:24.022557 batch 1 running loss 0.08233831384204493 loss 0.08233831384204493\n",
      "   2023-10-28 15:34:50.263366 batch 701 running loss 0.0821384867248294 loss 0.08102085096207379\n",
      "epoch: 17, AP: 0.2680392489395591\n",
      "   2023-10-28 15:35:09.296659 batch 1 running loss 0.08144170476389706 loss 0.08144170476389706\n",
      "   2023-10-28 15:37:45.416016 batch 701 running loss 0.08184298541881152 loss 0.08234294865877656\n",
      "epoch: 18, AP: 0.26803953330425\n",
      "   2023-10-28 15:38:14.450300 batch 1 running loss 0.08021414422740585 loss 0.08021414422740585\n",
      "   2023-10-28 15:41:52.309641 batch 701 running loss 0.07984678222252414 loss 0.07664960189041983\n",
      "epoch: 19, AP: 0.26986940525670305\n",
      "   2023-10-28 15:42:18.907400 batch 1 running loss 0.07790802276994066 loss 0.07790802276994066\n",
      "   2023-10-28 15:44:51.386396 batch 701 running loss 0.0695349658142553 loss 0.07061366271067815\n",
      "epoch: 20, AP: 0.25901708176337884\n",
      "   2023-10-28 15:45:10.731640 batch 1 running loss 0.06694391566065572 loss 0.06694391566065572\n",
      "   2023-10-28 15:48:00.896895 batch 701 running loss 0.06287543009125117 loss 0.06438169734520766\n",
      "epoch: 21, AP: 0.26703724599771184\n",
      "   2023-10-28 15:48:21.499908 batch 1 running loss 0.06412311300959095 loss 0.06412311300959095\n",
      "   2023-10-28 15:50:47.435459 batch 701 running loss 0.05575400927804224 loss 0.05361770160671424\n",
      "epoch: 22, AP: 0.27980558586932014\n",
      "   2023-10-28 15:51:56.129850 batch 1 running loss 0.055615772729742496 loss 0.055615772729742496\n",
      "   2023-10-28 15:55:37.352146 batch 701 running loss 0.05569323958337276 loss 0.057721288424258846\n",
      "epoch: 23, AP: 0.2867442524765554\n",
      "   2023-10-28 15:57:19.399271 batch 1 running loss 0.05610332657492312 loss 0.05610332657492312\n",
      "   2023-10-28 16:02:04.741422 batch 701 running loss 0.05127224592430298 loss 0.048825135960791854\n",
      "epoch: 24, AP: 0.2952726801636457\n",
      "   2023-10-28 16:04:24.747390 batch 1 running loss 0.04729093055092548 loss 0.04729093055092548\n",
      "   2023-10-28 16:09:11.608484 batch 701 running loss 0.050703942072761646 loss 0.051200612212368934\n",
      "epoch: 25, AP: 0.29972235750210013\n",
      "   2023-10-28 16:11:30.396779 batch 1 running loss 0.04822935885568749 loss 0.04822935885568749\n",
      "   2023-10-28 16:16:34.459305 batch 701 running loss 0.048167856513610705 loss 0.04924260479561199\n",
      "epoch: 26, AP: 0.3000451807040313\n",
      "   2023-10-28 16:18:53.710124 batch 1 running loss 0.04774598990543503 loss 0.04774598990543503\n",
      "   2023-10-28 16:23:41.404578 batch 701 running loss 0.0466367437292258 loss 0.04735023289847862\n",
      "epoch: 27, AP: 0.3044280734974305\n",
      "   2023-10-28 16:26:01.919002 batch 1 running loss 0.04929978427383391 loss 0.04929978427383391\n",
      "   2023-10-28 16:30:48.796746 batch 701 running loss 0.047529586082250636 loss 0.045391565931851785\n",
      "epoch: 28, AP: 0.30691484881616127\n",
      "   2023-10-28 16:33:10.196727 batch 1 running loss 0.045463209808918936 loss 0.045463209808918936\n",
      "   2023-10-28 16:38:12.042101 batch 701 running loss 0.04635756181669837 loss 0.049101506342565104\n",
      "epoch: 29, AP: 0.3083451576746439\n",
      "   2023-10-28 16:40:32.315939 batch 1 running loss 0.05447620758549476 loss 0.05447620758549476\n",
      "   2023-10-28 16:45:36.613651 batch 701 running loss 0.04748722661235904 loss 0.04903824585111405\n",
      "epoch: 30, AP: 0.2950562385787169\n",
      "   2023-10-28 16:46:15.821741 batch 1 running loss 0.0485364484284099 loss 0.0485364484284099\n",
      "   2023-10-28 16:51:19.238700 batch 701 running loss 0.04589199303643615 loss 0.04457854744829702\n",
      "epoch: 31, AP: 0.2986929241385677\n",
      "   2023-10-28 16:51:59.196645 batch 1 running loss 0.041481549101697546 loss 0.041481549101697546\n",
      "   2023-10-28 16:57:02.910228 batch 701 running loss 0.04595683518833455 loss 0.04506750871221854\n",
      "epoch: 32, AP: 0.3028415825030564\n",
      "   2023-10-28 16:57:42.011508 batch 1 running loss 0.043812115711904465 loss 0.043812115711904465\n",
      "   2023-10-28 17:02:45.135554 batch 701 running loss 0.044103579454779024 loss 0.0453708099389587\n",
      "epoch: 33, AP: 0.31545014931316495\n",
      "   2023-10-28 17:05:05.026394 batch 1 running loss 0.0384866841674496 loss 0.0384866841674496\n",
      "   2023-10-28 17:10:08.955126 batch 701 running loss 0.043387080458315436 loss 0.04367670419664105\n",
      "epoch: 34, AP: 0.31463081822620587\n",
      "   2023-10-28 17:10:48.520948 batch 1 running loss 0.040878099805992996 loss 0.040878099805992996\n",
      "   2023-10-28 17:15:52.414691 batch 701 running loss 0.04312295986767316 loss 0.04368606646480355\n",
      "epoch: 35, AP: 0.3131250867134714\n",
      "   2023-10-28 17:16:32.212000 batch 1 running loss 0.04782996953079843 loss 0.04782996953079843\n",
      "   2023-10-28 17:21:34.930350 batch 701 running loss 0.04394510369573948 loss 0.0433538307242185\n",
      "epoch: 36, AP: 0.3182997065596648\n",
      "   2023-10-28 17:23:55.112672 batch 1 running loss 0.04006163646892613 loss 0.04006163646892613\n",
      "   2023-10-28 17:28:57.251550 batch 701 running loss 0.04347162571590765 loss 0.050216583798326386\n",
      "epoch: 37, AP: 0.3290486297429105\n",
      "   2023-10-28 17:31:17.738104 batch 1 running loss 0.04200882393179306 loss 0.04200882393179306\n",
      "   2023-10-28 17:36:23.016076 batch 701 running loss 0.04147117624546827 loss 0.038779485379015544\n",
      "epoch: 38, AP: 0.3260585889323348\n",
      "   2023-10-28 17:37:01.596760 batch 1 running loss 0.0439358183953753 loss 0.0439358183953753\n",
      "   2023-10-28 17:42:04.311826 batch 701 running loss 0.04451073156435104 loss 0.044120867433234956\n",
      "epoch: 39, AP: 0.3259516214169481\n",
      "   2023-10-28 17:42:43.322864 batch 1 running loss 0.04699719782765843 loss 0.04699719782765843\n",
      "   2023-10-28 17:47:47.713728 batch 701 running loss 0.04315529011976388 loss 0.03984057315953726\n",
      "epoch: 40, AP: 0.31891294448243523\n",
      "   2023-10-28 17:48:27.032020 batch 1 running loss 0.0409532716288874 loss 0.0409532716288874\n",
      "   2023-10-28 17:53:23.404797 batch 701 running loss 0.04417433559196466 loss 0.04600789619766932\n",
      "epoch: 41, AP: 0.3108012678018457\n",
      "   2023-10-28 17:53:53.341426 batch 1 running loss 0.03958315976334115 loss 0.03958315976334115\n",
      "   2023-10-28 17:58:56.614460 batch 701 running loss 0.04257150134320358 loss 0.04766129785742601\n",
      "epoch: 42, AP: 0.31957433702061566\n",
      "   2023-10-28 17:59:34.204632 batch 1 running loss 0.04598233386785655 loss 0.04598233386785655\n",
      "   2023-10-28 18:04:21.808519 batch 701 running loss 0.04222153965071268 loss 0.03903933440761398\n",
      "epoch: 43, AP: 0.3230729277106251\n",
      "   2023-10-28 18:05:01.215763 batch 1 running loss 0.042182831145427535 loss 0.042182831145427535\n",
      "   2023-10-28 18:09:46.915420 batch 701 running loss 0.041873491752823115 loss 0.043498958251519726\n",
      "epoch: 44, AP: 0.3234160985075182\n",
      "   2023-10-28 18:10:26.116625 batch 1 running loss 0.03748267310852538 loss 0.03748267310852538\n",
      "   2023-10-28 18:15:12.639792 batch 701 running loss 0.04272004409077183 loss 0.04406197274127266\n",
      "epoch: 45, AP: 0.33054683370231996\n",
      "   2023-10-28 18:17:33.196739 batch 1 running loss 0.03800914260587973 loss 0.03800914260587973\n",
      "   2023-10-28 18:22:18.418592 batch 701 running loss 0.037992554537125744 loss 0.03407230136222594\n",
      "epoch: 46, AP: 0.3327770693453008\n",
      "   2023-10-28 18:24:38.552755 batch 1 running loss 0.03969726660483697 loss 0.03969726660483697\n",
      "   2023-10-28 18:29:42.596771 batch 701 running loss 0.03886684759541448 loss 0.03648549068269613\n",
      "epoch: 47, AP: 0.3351203162044314\n",
      "   2023-10-28 18:32:03.021829 batch 1 running loss 0.04314952947859507 loss 0.04314952947859507\n",
      "   2023-10-28 18:36:49.918907 batch 701 running loss 0.03995872617232595 loss 0.035075032559128916\n",
      "epoch: 48, AP: 0.33351435676062663\n",
      "   2023-10-28 18:37:29.314574 batch 1 running loss 0.04172339087311466 loss 0.04172339087311466\n",
      "   2023-10-28 18:42:15.447562 batch 701 running loss 0.03821986641968321 loss 0.03705567013927569\n",
      "epoch: 49, AP: 0.33327170387406047\n",
      "   2023-10-28 18:42:55.015245 batch 1 running loss 0.03909875894831416 loss 0.03909875894831416\n",
      "   2023-10-28 18:47:40.819613 batch 701 running loss 0.04219915570247379 loss 0.04329612839114283\n",
      "epoch: 50, AP: 0.32335566568212104\n",
      "   2023-10-28 18:48:20.716659 batch 1 running loss 0.03805514903281859 loss 0.03805514903281859\n",
      "   2023-10-28 18:53:24.646841 batch 701 running loss 0.04041419276012023 loss 0.03993529367176052\n",
      "epoch: 51, AP: 0.3192181135672142\n",
      "   2023-10-28 18:54:04.015665 batch 1 running loss 0.04084505916030573 loss 0.04084505916030573\n",
      "   2023-10-28 18:59:06.314334 batch 701 running loss 0.03986418742278305 loss 0.03755130147395075\n",
      "epoch: 52, AP: 0.3251423465974055\n",
      "   2023-10-28 18:59:46.213975 batch 1 running loss 0.03412670599273393 loss 0.03412670599273393\n",
      "   2023-10-28 19:04:47.733312 batch 701 running loss 0.040474784884184746 loss 0.04158211203602358\n",
      "epoch: 53, AP: 0.32464689346630915\n",
      "   2023-10-28 19:05:27.696641 batch 1 running loss 0.04631850916075403 loss 0.04631850916075403\n",
      "   2023-10-28 19:10:29.717492 batch 701 running loss 0.03952040599618 loss 0.040266679304714614\n",
      "epoch: 54, AP: 0.3285517189594389\n",
      "   2023-10-28 19:11:09.431630 batch 1 running loss 0.039799170749423685 loss 0.039799170749423685\n",
      "   2023-10-28 19:16:12.433747 batch 701 running loss 0.037576661262589614 loss 0.03520681092896566\n",
      "epoch: 55, AP: 0.3282510520740452\n",
      "   2023-10-28 19:16:51.696793 batch 1 running loss 0.03728134932697935 loss 0.03728134932697935\n",
      "   2023-10-28 19:21:36.496760 batch 701 running loss 0.03909394062012915 loss 0.0366958113246767\n",
      "epoch: 56, AP: 0.33286594962156524\n",
      "   2023-10-28 19:22:16.049140 batch 1 running loss 0.03815685879672115 loss 0.03815685879672115\n",
      "   2023-10-28 19:27:10.243612 batch 701 running loss 0.03847698378233097 loss 0.036262301702640665\n",
      "epoch: 57, AP: 0.3391756436125752\n",
      "   2023-10-28 19:29:21.815562 batch 1 running loss 0.03462476036307682 loss 0.03462476036307682\n",
      "   2023-10-28 19:34:11.426424 batch 701 running loss 0.03679893986399613 loss 0.03877922712854465\n",
      "epoch: 58, AP: 0.3376364400555447\n",
      "   2023-10-28 19:34:48.219069 batch 1 running loss 0.03813771939512933 loss 0.03813771939512933\n",
      "   2023-10-28 19:39:51.013198 batch 701 running loss 0.03872584480848266 loss 0.03955885246963803\n",
      "epoch: 59, AP: 0.3371345550962999\n",
      "   2023-10-28 19:40:30.317265 batch 1 running loss 0.036366260617934576 loss 0.036366260617934576\n",
      "   2023-10-28 19:45:31.940193 batch 701 running loss 0.03922481307065828 loss 0.03802429925002948\n",
      "epoch: 60, AP: 0.32290771952845854\n",
      "   2023-10-28 19:46:11.396808 batch 1 running loss 0.03788785059938893 loss 0.03788785059938893\n",
      "   2023-10-28 19:51:13.607457 batch 701 running loss 0.04136539385873714 loss 0.04000719637053768\n",
      "epoch: 61, AP: 0.32174221771014744\n",
      "   2023-10-28 19:51:52.928515 batch 1 running loss 0.036631156398855835 loss 0.036631156398855835\n",
      "   2023-10-28 19:56:54.111135 batch 701 running loss 0.03845682047399021 loss 0.03867255982008694\n",
      "epoch: 62, AP: 0.326324250867872\n",
      "   2023-10-28 19:57:33.711274 batch 1 running loss 0.0362760237062183 loss 0.0362760237062183\n",
      "   2023-10-28 20:02:35.999202 batch 701 running loss 0.037208014149757405 loss 0.031080746377523172\n",
      "epoch: 63, AP: 0.3309528678152919\n",
      "   2023-10-28 20:03:15.021838 batch 1 running loss 0.032295589571261496 loss 0.032295589571261496\n",
      "   2023-10-28 20:08:17.922898 batch 701 running loss 0.03970110879661225 loss 0.03754671995312042\n",
      "epoch: 64, AP: 0.3302943070640642\n",
      "   2023-10-28 20:08:57.409543 batch 1 running loss 0.03338183603178235 loss 0.03338183603178235\n",
      "   2023-10-28 20:14:00.635547 batch 701 running loss 0.03719469832033625 loss 0.03527530670928697\n",
      "epoch: 65, AP: 0.3303106264457698\n",
      "   2023-10-28 20:14:40.696698 batch 1 running loss 0.034436777559357395 loss 0.034436777559357395\n",
      "   2023-10-28 20:19:43.996755 batch 701 running loss 0.03871346544556435 loss 0.040372563728443944\n",
      "epoch: 66, AP: 0.3330868603854572\n",
      "   2023-10-28 20:20:23.518826 batch 1 running loss 0.039739336312144824 loss 0.039739336312144824\n",
      "   2023-10-28 20:25:25.210156 batch 701 running loss 0.03600796195153355 loss 0.03672042959011387\n",
      "epoch: 67, AP: 0.3355392294143873\n",
      "   2023-10-28 20:26:04.535171 batch 1 running loss 0.03726303303117695 loss 0.03726303303117695\n",
      "   2023-10-28 20:31:07.233024 batch 701 running loss 0.03627085173308452 loss 0.03875017424881699\n",
      "epoch: 68, AP: 0.33824275773880014\n",
      "   2023-10-28 20:31:46.996642 batch 1 running loss 0.03292081933154509 loss 0.03292081933154509\n",
      "   2023-10-28 20:36:50.196709 batch 701 running loss 0.03673565703014538 loss 0.03563106154520751\n",
      "epoch: 69, AP: 0.33739679824447943\n",
      "   2023-10-28 20:37:30.045481 batch 1 running loss 0.03477589586449997 loss 0.03477589586449997\n",
      "   2023-10-28 20:42:31.432051 batch 701 running loss 0.04032842351192312 loss 0.047086518345818645\n",
      "epoch: 70, AP: 0.32467980700950705\n",
      "   2023-10-28 20:43:10.648344 batch 1 running loss 0.03930496063865439 loss 0.03930496063865439\n",
      "   2023-10-28 20:48:12.715947 batch 701 running loss 0.037797135667673 loss 0.03641750813435807\n",
      "epoch: 71, AP: 0.33421971528349026\n",
      "   2023-10-28 20:48:52.149368 batch 1 running loss 0.040457596717211136 loss 0.040457596717211136\n",
      "   2023-10-28 20:53:55.196762 batch 701 running loss 0.03824409199981689 loss 0.041768782226916795\n",
      "epoch: 72, AP: 0.3227316877474813\n",
      "   2023-10-28 20:54:34.614624 batch 1 running loss 0.03439843535652776 loss 0.03439843535652776\n",
      "   2023-10-28 20:59:37.233820 batch 701 running loss 0.03718291414152482 loss 0.03319930681143163\n",
      "epoch: 73, AP: 0.3295094072628689\n",
      "   2023-10-28 21:00:17.205648 batch 1 running loss 0.03732736214165294 loss 0.03732736214165294\n",
      "   2023-10-28 21:05:20.099964 batch 701 running loss 0.03578870751639529 loss 0.03275676360589985\n",
      "epoch: 74, AP: 0.3326400435795135\n",
      "   2023-10-28 21:05:59.417133 batch 1 running loss 0.036059116044095674 loss 0.036059116044095674\n",
      "   2023-10-28 21:11:01.540983 batch 701 running loss 0.03615566060854471 loss 0.03542651979904832\n",
      "epoch: 75, AP: 0.33479463021127714\n",
      "   2023-10-28 21:11:40.638375 batch 1 running loss 0.03604040394401781 loss 0.03604040394401781\n",
      "   2023-10-28 21:16:44.419975 batch 701 running loss 0.03435962619520617 loss 0.034289201795200286\n",
      "epoch: 76, AP: 0.3327525525935872\n",
      "   2023-10-28 21:17:23.919747 batch 1 running loss 0.03164753133621079 loss 0.03164753133621079\n",
      "   2023-10-28 21:22:26.025977 batch 701 running loss 0.03438902561754876 loss 0.03270209446352387\n",
      "epoch: 77, AP: 0.3341163278593309\n",
      "   2023-10-28 21:23:05.533418 batch 1 running loss 0.03503993850360429 loss 0.03503993850360429\n",
      "   2023-10-28 21:28:08.708304 batch 701 running loss 0.03556323208904287 loss 0.03580641724761671\n",
      "epoch: 78, AP: 0.33524796323276745\n",
      "   2023-10-28 21:28:48.223327 batch 1 running loss 0.03003803428883245 loss 0.03003803428883245\n",
      "   2023-10-28 21:33:52.109512 batch 701 running loss 0.03380263238521329 loss 0.03067141319302069\n",
      "epoch: 79, AP: 0.33662562727837275\n",
      "   2023-10-28 21:34:31.431130 batch 1 running loss 0.037335441042589596 loss 0.037335441042589596\n",
      "   2023-10-28 21:39:35.720174 batch 701 running loss 0.036811701334783224 loss 0.0386432906172295\n",
      "epoch: 80, AP: 0.32928200752426817\n",
      "   2023-10-28 21:40:15.596663 batch 1 running loss 0.03360736492428151 loss 0.03360736492428151\n",
      "   2023-10-28 21:45:19.537198 batch 701 running loss 0.03786525605090457 loss 0.03784205851261223\n",
      "epoch: 81, AP: 0.32830330200068547\n",
      "   2023-10-28 21:45:58.950359 batch 1 running loss 0.036865931416721454 loss 0.036865931416721454\n",
      "   2023-10-28 21:51:02.209056 batch 701 running loss 0.03619420624989351 loss 0.032601962792298544\n",
      "epoch: 82, AP: 0.32408843480880484\n",
      "   2023-10-28 21:51:41.524529 batch 1 running loss 0.03545974554099543 loss 0.03545974554099543\n",
      "   2023-10-28 21:56:45.214320 batch 701 running loss 0.03481159270385523 loss 0.0330508498323529\n",
      "epoch: 83, AP: 0.32465744698706145\n",
      "   2023-10-28 21:57:24.239037 batch 1 running loss 0.033687487133856345 loss 0.033687487133856345\n",
      "   2023-10-28 22:02:29.014631 batch 701 running loss 0.0340094466540853 loss 0.03226170225773439\n",
      "epoch: 84, AP: 0.3271151632346374\n",
      "   2023-10-28 22:03:08.015270 batch 1 running loss 0.033850466845843113 loss 0.033850466845843113\n",
      "   2023-10-28 22:08:11.111287 batch 701 running loss 0.033304611883007706 loss 0.03206565466262121\n",
      "epoch: 85, AP: 0.3290954992577113\n",
      "   2023-10-28 22:08:50.418064 batch 1 running loss 0.030929702972062295 loss 0.030929702972062295\n",
      "   2023-10-28 22:13:53.216047 batch 701 running loss 0.03477199421446971 loss 0.04035085371784576\n",
      "epoch: 86, AP: 0.3293072770783902\n",
      "   2023-10-28 22:14:32.816635 batch 1 running loss 0.03297599425846484 loss 0.03297599425846484\n",
      "   2023-10-28 22:19:29.918919 batch 701 running loss 0.033565691458427006 loss 0.03366360845268239\n",
      "epoch: 87, AP: 0.3336784716254421\n",
      "   2023-10-28 22:20:04.134209 batch 1 running loss 0.027842720858714892 loss 0.027842720858714892\n",
      "   2023-10-28 22:23:50.499538 batch 701 running loss 0.03456958617504668 loss 0.033318929337299574\n",
      "epoch: 88, AP: 0.33077732655678227\n",
      "   2023-10-28 22:24:20.263530 batch 1 running loss 0.03062173754079231 loss 0.03062173754079231\n",
      "   2023-10-28 22:28:06.561630 batch 701 running loss 0.03350915362275784 loss 0.033814056372702045\n",
      "epoch: 89, AP: 0.3306857930183214\n",
      "   2023-10-28 22:28:35.851117 batch 1 running loss 0.0324426501833642 loss 0.0324426501833642\n",
      "   2023-10-28 22:30:08.005440 batch 701 running loss 0.033974615915853046 loss 0.03109075964650221\n",
      "epoch: 90, AP: 0.32002224424779013\n",
      "   2023-10-28 22:30:17.976080 batch 1 running loss 0.03698912403065399 loss 0.03698912403065399\n",
      "   2023-10-28 22:31:34.220746 batch 701 running loss 0.034745077911178736 loss 0.030405798295753593\n",
      "epoch: 91, AP: 0.32281796096793497\n",
      "   2023-10-28 22:31:44.172028 batch 1 running loss 0.03304151406296041 loss 0.03304151406296041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "from timm.loss import AsymmetricLossMultiLabel\n",
    "from transformers import set_seed\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "\n",
    "model = Network()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "epochs = 150\n",
    "model = model.to(CUDA_DEV)\n",
    "criterion = criterion.to(CUDA_DEV)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, eta_min=1e-7)\n",
    "\n",
    "best = 0\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_epoch(model, train_dataloader, criterion, optimizer, scheduler, print_loss=True, iteration_step=700, epoch=epoch)\n",
    "    track_idxs, predictions, targets = predict_train(model, val_dataloader)\n",
    "    ap = sklearn.metrics.average_precision_score(targets, predictions)\n",
    "    print(f\"epoch: {epoch}, AP: {ap}\")\n",
    "    if (ap > best):\n",
    "        best = ap\n",
    "        best_model = model\n",
    "        if (epoch > 20):\n",
    "            track_idxs, predictions = predict(model, test_dataloader)\n",
    "            for i, c in enumerate(predictions.argmax(-1)):\n",
    "                probs = np.array([1 + dict_tags[c].get(t, 0) for t in np.arange(predictions.shape[1])])\n",
    "                probs[c] = 2\n",
    "                predictions[i] = predictions[i] * probs\n",
    "                predictions[i] /= predictions[i].sum()\n",
    "                        \n",
    "            predictions_df = pd.DataFrame([\n",
    "                {'track': track, 'prediction': ','.join([str(p) for p in probs])}\n",
    "                for track, probs in zip(track_idxs, predictions)\n",
    "            ])\n",
    "            predictions_df.to_csv(f'subs/prediction_best.csv', index=False)\n",
    "            torch.save(best_model.state_dict(), f'models/models_best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa898f51-c075-47fc-95a0-b4f95ed655b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP: 0.3199148484354201\n",
      "POST AP: 0.32061250757510396\n",
      "POST-1 AP: 0.3215476797234166\n"
     ]
    }
   ],
   "source": [
    "track_idxs, predictions, targets = predict_train(best_model, val_dataloader)\n",
    "ap = sklearn.metrics.average_precision_score(targets, predictions)\n",
    "print(f\"AP: {ap}\")\n",
    "\n",
    "for i, c in enumerate(predictions.argmax(-1)):\n",
    "    probs = np.array([1 + dict_tags[c].get(t, 0) for t in np.arange(predictions.shape[1])])\n",
    "    probs[c] = 2\n",
    "    predictions[i] = predictions[i] * probs\n",
    "    predictions[i] /= predictions[i].sum()\n",
    "ap = sklearn.metrics.average_precision_score(targets, predictions)\n",
    "\n",
    "print(f\"POST AP: {ap}\")\n",
    "\n",
    "for i, c in enumerate(predictions.argmax(-1)):\n",
    "    probs = np.array([1 + dict_tags[c].get(t, 0) for t in np.arange(predictions.shape[1])])\n",
    "    probs[c] = 2\n",
    "    predictions[i] = predictions[i] * probs\n",
    "    predictions[i] /= predictions[i].sum()\n",
    "ap = sklearn.metrics.average_precision_score(targets, predictions)\n",
    "\n",
    "print(f\"POST-1 AP: {ap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4daf63c5-45b4-4cb8-99f1-534fc8203caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_idxs, predictions = predict(best_model, test_dataloader)\n",
    "\n",
    "for i, c in enumerate(predictions.argmax(-1)):\n",
    "    probs = np.array([1 + dict_tags[c].get(t, 0) for t in np.arange(predictions.shape[1])])\n",
    "    probs[c] = 2\n",
    "    predictions[i] = predictions[i] * probs\n",
    "    predictions[i] /= predictions[i].sum()\n",
    "    \n",
    "for i, c in enumerate(predictions.argmax(-1)):\n",
    "    probs = np.array([1 + dict_tags[c].get(t, 0) for t in np.arange(predictions.shape[1])])\n",
    "    probs[c] = 2\n",
    "    predictions[i] = predictions[i] * probs\n",
    "    predictions[i] /= predictions[i].sum()\n",
    "\n",
    "predictions_df = pd.DataFrame([\n",
    "    {'track': track, 'prediction': ','.join([str(p) for p in probs])}\n",
    "    for track, probs in zip(track_idxs, predictions)\n",
    "])\n",
    "predictions_df.to_csv(f'prediction_best_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ae520e-08d1-4a77-85e3-d0023c8fc8a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
