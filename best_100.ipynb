{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "340aceb6-4360-49ac-8381-bb42055d6146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.remove('/home/jovyan/.imgenv-lm-poly-0/lib/python3.7/site-packages')\n",
    "os.environ['PYTHONPATH'] = '/home/user/conda/envs/ya/lib/python3.10/site-packages'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32801790-6619-4141-b909-a70de5e00071",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_DEV = 0\n",
    "NUM_TAGS = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29f35090-eb3d-464b-b46f-de30c4d42d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "144deed2-ca15-41aa-9225-6edf01d1003e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "tags = [[int(i) for i in x.split(',')] for x in df_train.tags.values]\n",
    "dict_tags = {}\n",
    "for cls_tags in tags:\n",
    "    for c in cls_tags:\n",
    "        if c not in dict_tags.keys():\n",
    "            dict_tags[c] = Counter(cls_tags)\n",
    "        else:\n",
    "            dict_tags[c].update(Counter(cls_tags))\n",
    "            \n",
    "for tag in dict_tags.keys():\n",
    "    del dict_tags[tag][tag]\n",
    "    n = np.sum(list(dict_tags[tag].values()))\n",
    "    for t in dict_tags[tag].keys():\n",
    "        dict_tags[tag][t] = dict_tags[tag][t]/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b46b7d4b-785e-4187-b2bc-6c620c7b8cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 76715/76715 [03:17<00:00, 389.03it/s]\n"
     ]
    }
   ],
   "source": [
    "track_idx2embeds = {}\n",
    "for fn in tqdm(glob('track_embeddings/*')):\n",
    "    name = fn.split('/')[1].split('.')[0]\n",
    "    if name == \"track_embeddings\":\n",
    "        continue\n",
    "    track_idx = int(name)\n",
    "    embeds = np.load(fn)\n",
    "    track_idx2embeds[track_idx] = embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d1b5cae-60ae-4584-a6bb-4f6b833929aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaggingDataset(Dataset):\n",
    "    def __init__(self, df, aug=0, testing=False):\n",
    "        self.df = df\n",
    "        self.testing = testing\n",
    "        self.aug = aug\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        track_idx = row.track\n",
    "        embeds = track_idx2embeds[track_idx]\n",
    "        if self.testing:\n",
    "            return track_idx, embeds\n",
    "        tags = [int(x) for x in row.tags.split(',')]\n",
    "        target = np.zeros(NUM_TAGS)\n",
    "        target[tags] = 1\n",
    "        \n",
    "        if np.random.choice([0, 1], p=[1 - self.aug, self.aug]):\n",
    "            s = np.random.uniform(0.0, 0.4)\n",
    "            e = np.random.uniform(s+0.1, 1)\n",
    "            s = int(s * embeds.shape[0])\n",
    "            e = int(e * embeds.shape[0])\n",
    "            embeds = embeds[s:e]\n",
    "        \n",
    "        return track_idx, embeds, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca9ac5cf-a481-4918-bbeb-ecf077c681ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TaggingDataset(df_train[:-1000], aug=0.6)\n",
    "val_dataset = TaggingDataset(df_train[-1000:])\n",
    "\n",
    "test_dataset = TaggingDataset(df_test, testing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31c659b7-ee4b-44da-a715-b7abced07279",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, emb_dim=768, mult=4, p=0.0):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(emb_dim, emb_dim * mult),\n",
    "            nn.Dropout(p),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(emb_dim * mult, emb_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "    \n",
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Sequential(\n",
    "            nn.Linear(embedding_size, embedding_size),\n",
    "            nn.LayerNorm(embedding_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embedding_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_logits = self.attn(x)\n",
    "        if mask is not None:\n",
    "            attn_logits[mask] = -float('inf')\n",
    "        attn_weights = torch.softmax(attn_logits, dim=1)\n",
    "        x = x * attn_weights\n",
    "        x = x.sum(dim=1)\n",
    "        return x\n",
    "    \n",
    "class Network(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes = NUM_TAGS,\n",
    "        input_dim = 768,\n",
    "        hidden_dim = 512\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.position_enc = nn.Embedding(128, input_dim, padding_idx=-1) \n",
    "        self.proj = FeedForward(input_dim)\n",
    "        self.bn = nn.BatchNorm1d(input_dim)\n",
    "        self.ln = nn.LayerNorm(input_dim)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=768, nhead=12, activation=\"gelu\", batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=6)\n",
    "        self.poooling = AttentionPooling(input_dim)\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "               \n",
    "    def forward(self, embeds):\n",
    "        embeds = self.proj(embeds)\n",
    "        src_key_padding_mask = (embeds.mean(-1) == -1)\n",
    "        embeds = self.ln(embeds)\n",
    "        x = self.transformer_encoder(embeds, src_key_padding_mask=src_key_padding_mask)\n",
    "        x = self.bn(self.poooling(x, mask=src_key_padding_mask))\n",
    "        outs = self.fc(x)\n",
    "        return outs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c70bf034-7966-4f44-9f2e-dcaf0f8a8184",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, scheduler, print_loss=True, iteration_step=100, epoch=0):\n",
    "    model.train()\n",
    "    running_loss = None\n",
    "    alpha = 0.8\n",
    "    iters = len(loader)\n",
    "    for iteration,data in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        track_idxs, embeds, target = data\n",
    "        embeds = [x.to(CUDA_DEV) for x in embeds]\n",
    "        embeds = pad_sequence(embeds, padding_value=-1, batch_first=True)[:, :100, :]\n",
    "        target = target.to(CUDA_DEV)\n",
    "        pred_logits = model(embeds)\n",
    "        pred_probs = torch.sigmoid(pred_logits)\n",
    "        ce_loss = criterion(pred_logits, target)\n",
    "            \n",
    "        ce_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        scheduler.step(epoch + iteration / iters)\n",
    "        \n",
    "        if running_loss is None:\n",
    "            running_loss = ce_loss.item()\n",
    "        else:\n",
    "            running_loss = alpha * running_loss + (1 - alpha) * ce_loss.item()\n",
    "        if (iteration % iteration_step == 0) and print_loss:\n",
    "            print('   {} batch {} running loss {} loss {}'.format(\n",
    "                datetime.now(), iteration + 1, running_loss, ce_loss.item()\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e624b0b-8daf-4702-a5de-c667fcd53121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, loader):\n",
    "    model.eval()\n",
    "    track_idxs = []\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            track_idx, embeds = data\n",
    "            embeds = [x.to(CUDA_DEV) for x in embeds]\n",
    "            embeds = pad_sequence(embeds, padding_value=-1, batch_first=True)[:, :100, :]\n",
    "            pred_logits = model(embeds)\n",
    "            pred_probs = torch.sigmoid(pred_logits)\n",
    "            predictions.append(pred_probs.cpu().numpy())\n",
    "            track_idxs.append(track_idx.numpy())\n",
    "    predictions = np.vstack(predictions)\n",
    "    track_idxs = np.vstack(track_idxs).ravel()\n",
    "    return track_idxs, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37c01c5a-6d40-47d3-a535-d3349b41139e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def predict_train(model, loader):\n",
    "    model.eval()\n",
    "    track_idxs = []\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            track_idx, embeds, target = data\n",
    "            embeds = [x.to(CUDA_DEV) for x in embeds]\n",
    "            embeds = pad_sequence(embeds, padding_value=-1, batch_first=True)[:, :100, :]\n",
    "            pred_logits = model(embeds)\n",
    "            pred_probs = torch.sigmoid(pred_logits)\n",
    "            predictions.append(pred_probs.cpu().numpy())\n",
    "            track_idxs.append(track_idx.numpy())\n",
    "            targets.append(target.numpy())\n",
    "    predictions = np.vstack(predictions)\n",
    "    targets = np.vstack(targets)\n",
    "    track_idxs = np.vstack(track_idxs).ravel()\n",
    "    return track_idxs, predictions, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4decde5-005a-4820-804c-cf4d110c799e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(b):\n",
    "    track_idxs = torch.from_numpy(np.vstack([x[0] for x in b]))\n",
    "    embeds = [torch.from_numpy(x[1]) for x in b]\n",
    "    targets = np.vstack([x[2] for x in b])\n",
    "    targets = torch.from_numpy(targets)\n",
    "    return track_idxs, embeds, targets\n",
    "\n",
    "def collate_fn_test(b):\n",
    "    track_idxs = torch.from_numpy(np.vstack([x[0] for x in b]))\n",
    "    embeds = [torch.from_numpy(x[1]) for x in b]\n",
    "    return track_idxs, embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62124a85-3bcc-4eb8-a0d7-931c8685d1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=128, shuffle=False, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False, collate_fn=collate_fn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448ff0ee-2a61-4761-8d54-8b555bca7a09",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2023-10-29 09:10:16.576684 batch 1 running loss 0.7312761835159876 loss 0.7312761835159876\n",
      "   2023-10-29 09:13:45.272840 batch 701 running loss 0.6817180257567911 loss 0.6818873246007087\n",
      "epoch: 0, AP: 0.1059563134121971\n",
      "   2023-10-29 09:14:12.124967 batch 1 running loss 0.6795911946504924 loss 0.6795911946504924\n",
      "   2023-10-29 09:17:40.982385 batch 701 running loss 0.6451877480641999 loss 0.64542194709196\n",
      "epoch: 1, AP: 0.1482543048193352\n",
      "   2023-10-29 09:18:07.101084 batch 1 running loss 0.6374280626241609 loss 0.6374280626241609\n",
      "   2023-10-29 09:21:35.730374 batch 701 running loss 0.5721667922282355 loss 0.573411288884866\n",
      "epoch: 2, AP: 0.16306225529576257\n",
      "   2023-10-29 09:22:02.353652 batch 1 running loss 0.5630295297317685 loss 0.5630295297317685\n",
      "   2023-10-29 09:25:30.295562 batch 701 running loss 0.4899839137965865 loss 0.4903315536754542\n",
      "epoch: 3, AP: 0.17095700058325192\n",
      "   2023-10-29 09:25:56.971703 batch 1 running loss 0.48355425828523546 loss 0.48355425828523546\n",
      "   2023-10-29 09:29:25.630627 batch 701 running loss 0.42058092244505785 loss 0.4196955039064738\n",
      "epoch: 4, AP: 0.1938801388872178\n",
      "   2023-10-29 09:29:52.082026 batch 1 running loss 0.41512450324845274 loss 0.41512450324845274\n",
      "   2023-10-29 09:33:20.475819 batch 701 running loss 0.372372608680802 loss 0.37173915434181226\n",
      "epoch: 5, AP: 0.20530549922375496\n",
      "   2023-10-29 09:33:47.385782 batch 1 running loss 0.368701500170242 loss 0.368701500170242\n",
      "   2023-10-29 09:37:15.084839 batch 701 running loss 0.3410944783586191 loss 0.3412508507836921\n",
      "epoch: 6, AP: 0.21152243435991924\n",
      "   2023-10-29 09:37:42.093734 batch 1 running loss 0.33899204474392264 loss 0.33899204474392264\n",
      "   2023-10-29 09:41:10.881388 batch 701 running loss 0.32468963254049743 loss 0.32532953433537326\n",
      "epoch: 7, AP: 0.21364897857851078\n",
      "   2023-10-29 09:41:37.246972 batch 1 running loss 0.3233478612742655 loss 0.3233478612742655\n",
      "   2023-10-29 09:45:05.518317 batch 701 running loss 0.318340958402664 loss 0.32102842378991586\n",
      "epoch: 8, AP: 0.21642499164636586\n",
      "   2023-10-29 09:45:31.585120 batch 1 running loss 0.3151145639188426 loss 0.3151145639188426\n",
      "   2023-10-29 09:49:00.403234 batch 701 running loss 0.3156466124677755 loss 0.3162621517890264\n",
      "epoch: 9, AP: 0.21595980463210887\n",
      "   2023-10-29 09:49:27.248123 batch 1 running loss 0.31695749050658545 loss 0.31695749050658545\n",
      "   2023-10-29 09:52:55.270506 batch 701 running loss 0.24066611667544535 loss 0.24077863766135188\n",
      "epoch: 10, AP: 0.20653305290098595\n",
      "   2023-10-29 09:53:21.761600 batch 1 running loss 0.2322180667244662 loss 0.2322180667244662\n",
      "   2023-10-29 09:56:50.269063 batch 701 running loss 0.17881907429852217 loss 0.1807144660731681\n",
      "epoch: 11, AP: 0.21068094655963376\n",
      "   2023-10-29 09:57:16.968686 batch 1 running loss 0.17531640853091668 loss 0.17531640853091668\n",
      "   2023-10-29 10:00:45.629236 batch 701 running loss 0.13928449676848156 loss 0.14007509638511273\n",
      "epoch: 12, AP: 0.22210668730141075\n",
      "   2023-10-29 10:01:12.161241 batch 1 running loss 0.13473207060783426 loss 0.13473207060783426\n",
      "   2023-10-29 10:04:40.896735 batch 701 running loss 0.1148316952312172 loss 0.11481826135723094\n",
      "epoch: 13, AP: 0.24514921143245197\n",
      "   2023-10-29 10:05:07.971304 batch 1 running loss 0.11157939520139237 loss 0.11157939520139237\n",
      "   2023-10-29 10:08:36.296777 batch 701 running loss 0.09925997568596956 loss 0.10010003412860707\n",
      "epoch: 14, AP: 0.249555494022334\n",
      "   2023-10-29 10:09:03.177463 batch 1 running loss 0.09706127671808468 loss 0.09706127671808468\n",
      "   2023-10-29 10:12:31.259566 batch 701 running loss 0.0910508943505472 loss 0.09199856525366933\n",
      "epoch: 15, AP: 0.2623848731185668\n",
      "   2023-10-29 10:12:57.944625 batch 1 running loss 0.08806537859606434 loss 0.08806537859606434\n",
      "   2023-10-29 10:17:10.518375 batch 701 running loss 0.08518548777762436 loss 0.0829562263695842\n",
      "epoch: 16, AP: 0.2672919918885185\n",
      "   2023-10-29 10:17:44.820755 batch 1 running loss 0.08170951333901938 loss 0.08170951333901938\n",
      "   2023-10-29 10:21:31.512475 batch 701 running loss 0.08170087044587279 loss 0.0804815011944448\n",
      "epoch: 17, AP: 0.2677919249224202\n",
      "   2023-10-29 10:21:55.903909 batch 1 running loss 0.08091865466474946 loss 0.08091865466474946\n",
      "   2023-10-29 10:26:35.919862 batch 701 running loss 0.08138673697815238 loss 0.08223493134270865\n",
      "epoch: 18, AP: 0.26976679970259104\n",
      "   2023-10-29 10:27:04.547102 batch 1 running loss 0.07971642179866532 loss 0.07971642179866532\n",
      "   2023-10-29 10:30:15.080080 batch 701 running loss 0.07929300067081707 loss 0.07611110272829436\n",
      "epoch: 19, AP: 0.272907369652886\n",
      "   2023-10-29 10:30:41.767360 batch 1 running loss 0.07781863019085342 loss 0.07781863019085342\n",
      "   2023-10-29 10:33:51.808616 batch 701 running loss 0.06921168579943428 loss 0.07010343823875353\n",
      "epoch: 20, AP: 0.26029418200808097\n",
      "   2023-10-29 10:34:18.596677 batch 1 running loss 0.06610789974363109 loss 0.06610789974363109\n",
      "   2023-10-29 10:39:01.135447 batch 701 running loss 0.06260887249022198 loss 0.06440065548839602\n",
      "epoch: 21, AP: 0.27123573749876273\n",
      "   2023-10-29 10:39:36.915778 batch 1 running loss 0.06369491812971395 loss 0.06369491812971395\n",
      "   2023-10-29 10:43:03.059288 batch 701 running loss 0.05549167568564121 loss 0.053093201795808787\n",
      "epoch: 22, AP: 0.28242754778550105\n",
      "   2023-10-29 10:44:15.696716 batch 1 running loss 0.05550230255086318 loss 0.05550230255086318\n",
      "   2023-10-29 10:47:38.224010 batch 701 running loss 0.055503539545525005 loss 0.05746715155714209\n",
      "epoch: 23, AP: 0.2853708568466214\n",
      "   2023-10-29 10:48:48.004967 batch 1 running loss 0.05576704462779643 loss 0.05576704462779643\n",
      "   2023-10-29 10:52:32.011046 batch 701 running loss 0.05089495132891534 loss 0.04850602418504657\n",
      "epoch: 24, AP: 0.30116414340966136\n",
      "   2023-10-29 10:54:33.417210 batch 1 running loss 0.04737443518114692 loss 0.04737443518114692\n",
      "   2023-10-29 10:58:40.042837 batch 701 running loss 0.050593212465477114 loss 0.0512121135960387\n",
      "epoch: 25, AP: 0.3027138135333382\n",
      "   2023-10-29 11:00:07.752431 batch 1 running loss 0.04795878412744668 loss 0.04795878412744668\n",
      "   2023-10-29 11:04:12.600390 batch 701 running loss 0.048016264335081806 loss 0.04934351681765653\n",
      "epoch: 26, AP: 0.3002506506161307\n",
      "   2023-10-29 11:04:38.726845 batch 1 running loss 0.04731261343322313 loss 0.04731261343322313\n",
      "   2023-10-29 11:08:11.264520 batch 701 running loss 0.04847193641762035 loss 0.04961951326652189\n",
      "epoch: 27, AP: 0.3032542671867143\n",
      "   2023-10-29 11:09:58.113658 batch 1 running loss 0.047483662369721236 loss 0.047483662369721236\n",
      "   2023-10-29 11:13:30.365671 batch 701 running loss 0.047015769083242245 loss 0.046475994120731556\n",
      "epoch: 28, AP: 0.30522073509308634\n",
      "   2023-10-29 11:14:53.116646 batch 1 running loss 0.04912784312936336 loss 0.04912784312936336\n",
      "   2023-10-29 11:18:26.109975 batch 701 running loss 0.04716576764257865 loss 0.05141989735362784\n",
      "epoch: 29, AP: 0.30577967400409795\n",
      "   2023-10-29 11:20:35.812019 batch 1 running loss 0.04983938613772575 loss 0.04983938613772575\n",
      "   2023-10-29 11:27:08.128659 batch 701 running loss 0.04874444270471809 loss 0.04953384659222593\n",
      "epoch: 30, AP: 0.2925873133857577\n",
      "   2023-10-29 11:27:58.456021 batch 1 running loss 0.04816064592968228 loss 0.04816064592968228\n",
      "   2023-10-29 11:34:34.611383 batch 701 running loss 0.045780409287464255 loss 0.05296001580558496\n",
      "epoch: 31, AP: 0.2924355258600627\n",
      "   2023-10-29 11:35:25.951501 batch 1 running loss 0.04539941502581557 loss 0.04539941502581557\n",
      "   2023-10-29 11:41:56.231485 batch 701 running loss 0.04370664575798993 loss 0.0471263906592192\n",
      "epoch: 32, AP: 0.30384008352952363\n",
      "   2023-10-29 11:42:47.637589 batch 1 running loss 0.042272714444915094 loss 0.042272714444915094\n",
      "   2023-10-29 11:49:24.996740 batch 701 running loss 0.04470640102335575 loss 0.04897266116701804\n",
      "epoch: 33, AP: 0.3046403201753918\n",
      "   2023-10-29 11:50:15.751921 batch 1 running loss 0.03957942523274127 loss 0.03957942523274127\n",
      "   2023-10-29 11:56:51.742555 batch 701 running loss 0.043791910580343124 loss 0.04627921428490156\n",
      "epoch: 34, AP: 0.3117744932996114\n",
      "   2023-10-29 11:59:34.245129 batch 1 running loss 0.04907536463783346 loss 0.04907536463783346\n",
      "   2023-10-29 12:06:08.609610 batch 701 running loss 0.041541880988850415 loss 0.038683495186791816\n",
      "epoch: 35, AP: 0.3147723886066077\n",
      "   2023-10-29 12:08:48.617204 batch 1 running loss 0.04813371760273566 loss 0.04813371760273566\n",
      "   2023-10-29 12:15:17.216295 batch 701 running loss 0.04407546370472918 loss 0.04383531426562781\n",
      "epoch: 36, AP: 0.3156306249742604\n",
      "   2023-10-29 12:18:01.015216 batch 1 running loss 0.03944774844528931 loss 0.03944774844528931\n",
      "   2023-10-29 12:24:37.118830 batch 701 running loss 0.04306462235152655 loss 0.04972990143366118\n",
      "epoch: 37, AP: 0.3207619370762058\n",
      "   2023-10-29 12:27:21.449527 batch 1 running loss 0.042528537580060366 loss 0.042528537580060366\n",
      "   2023-10-29 12:33:55.299899 batch 701 running loss 0.04130076466837565 loss 0.03822479224830211\n",
      "epoch: 38, AP: 0.3193068988672418\n",
      "   2023-10-29 12:34:46.812720 batch 1 running loss 0.043727373583489104 loss 0.043727373583489104\n",
      "   2023-10-29 12:41:23.355012 batch 701 running loss 0.044372120130256765 loss 0.04457604702347462\n",
      "epoch: 39, AP: 0.3212213136404759\n",
      "   2023-10-29 12:44:07.919767 batch 1 running loss 0.047491578551493774 loss 0.047491578551493774\n",
      "   2023-10-29 12:50:45.811751 batch 701 running loss 0.042777204231914105 loss 0.038131189274199784\n",
      "epoch: 40, AP: 0.3098747298425928\n",
      "   2023-10-29 12:51:37.753747 batch 1 running loss 0.04738036193536743 loss 0.04738036193536743\n",
      "   2023-10-29 12:58:14.013849 batch 701 running loss 0.04185388938859151 loss 0.046960817415687695\n",
      "epoch: 41, AP: 0.3127209688697814\n",
      "   2023-10-29 12:59:06.216717 batch 1 running loss 0.046267567314311364 loss 0.046267567314311364\n",
      "   2023-10-29 13:05:37.249639 batch 701 running loss 0.04092791572486014 loss 0.04058208173005795\n",
      "epoch: 42, AP: 0.30865034491896726\n",
      "   2023-10-29 13:06:28.931305 batch 1 running loss 0.044490821956591375 loss 0.044490821956591375\n",
      "   2023-10-29 13:13:04.915607 batch 701 running loss 0.040161759413445956 loss 0.039231956348443564\n",
      "epoch: 43, AP: 0.31843724004197926\n",
      "   2023-10-29 13:13:57.209130 batch 1 running loss 0.04132837638458908 loss 0.04132837638458908\n",
      "   2023-10-29 13:20:31.443182 batch 701 running loss 0.04145671065081413 loss 0.04574665615963647\n",
      "epoch: 44, AP: 0.31545251682193304\n",
      "   2023-10-29 13:21:19.815041 batch 1 running loss 0.03593410834829169 loss 0.03593410834829169\n",
      "   2023-10-29 13:27:56.096698 batch 701 running loss 0.040486742828422385 loss 0.03736237936478448\n",
      "epoch: 45, AP: 0.31974726494407224\n",
      "   2023-10-29 13:28:48.412261 batch 1 running loss 0.0367531215774386 loss 0.0367531215774386\n",
      "   2023-10-29 13:35:23.331443 batch 701 running loss 0.03778639026692569 loss 0.03414822290508068\n",
      "epoch: 46, AP: 0.3256221442922478\n",
      "   2023-10-29 13:38:07.117981 batch 1 running loss 0.039303839513253 loss 0.039303839513253\n",
      "   2023-10-29 13:44:41.615235 batch 701 running loss 0.03875924283704191 loss 0.0370108177365458\n",
      "epoch: 47, AP: 0.32647522077808333\n",
      "   2023-10-29 13:47:25.217661 batch 1 running loss 0.04294409921304532 loss 0.04294409921304532\n",
      "   2023-10-29 13:53:06.359547 batch 701 running loss 0.03944241528105734 loss 0.03463396896589266\n",
      "epoch: 48, AP: 0.3249728352866581\n",
      "   2023-10-29 13:53:45.696654 batch 1 running loss 0.040335419226462 loss 0.040335419226462\n",
      "   2023-10-29 13:59:34.534174 batch 701 running loss 0.0379493207925039 loss 0.036750410746635875\n",
      "epoch: 49, AP: 0.3248255670075812\n",
      "   2023-10-29 14:00:26.237762 batch 1 running loss 0.0379828433879521 loss 0.0379828433879521\n",
      "   2023-10-29 14:06:56.826934 batch 701 running loss 0.04209430742336678 loss 0.043384547961909936\n",
      "epoch: 50, AP: 0.3181465677129287\n",
      "   2023-10-29 14:07:48.512942 batch 1 running loss 0.03692558196838158 loss 0.03692558196838158\n",
      "   2023-10-29 14:14:24.796661 batch 701 running loss 0.03997886184095625 loss 0.03995191560921296\n",
      "epoch: 51, AP: 0.3218776086060149\n",
      "   2023-10-29 14:15:15.359160 batch 1 running loss 0.041265908303688814 loss 0.041265908303688814\n",
      "   2023-10-29 14:21:51.132145 batch 701 running loss 0.0394607682629874 loss 0.03696787879987584\n",
      "epoch: 52, AP: 0.31517353375180623\n",
      "   2023-10-29 14:22:39.151568 batch 1 running loss 0.033971371966748376 loss 0.033971371966748376\n",
      "   2023-10-29 14:29:16.128578 batch 701 running loss 0.040029501731201254 loss 0.04221827950625878\n",
      "epoch: 53, AP: 0.3194658215790113\n",
      "   2023-10-29 14:30:07.414633 batch 1 running loss 0.04494707109526613 loss 0.04494707109526613\n",
      "   2023-10-29 14:36:42.118192 batch 701 running loss 0.0388842331120504 loss 0.03942008913482181\n",
      "epoch: 54, AP: 0.32785657921337036\n",
      "   2023-10-29 14:39:24.196764 batch 1 running loss 0.036725697312121885 loss 0.036725697312121885\n",
      "   2023-10-29 14:46:01.012400 batch 701 running loss 0.03734030364592233 loss 0.035934009322821536\n",
      "epoch: 55, AP: 0.32398458644746597\n",
      "   2023-10-29 14:46:53.196806 batch 1 running loss 0.040167891708492 loss 0.040167891708492\n",
      "   2023-10-29 14:53:30.331987 batch 701 running loss 0.03725746064027843 loss 0.03986315333089055\n",
      "epoch: 56, AP: 0.32724351907400373\n",
      "   2023-10-29 14:54:17.832078 batch 1 running loss 0.03413419197393741 loss 0.03413419197393741\n",
      "   2023-10-29 15:00:56.112527 batch 701 running loss 0.03792122977122048 loss 0.03640482242483578\n",
      "epoch: 57, AP: 0.3281459521696738\n",
      "   2023-10-29 15:03:39.013466 batch 1 running loss 0.03779729542141552 loss 0.03779729542141552\n",
      "   2023-10-29 15:10:10.417946 batch 701 running loss 0.03877875675525584 loss 0.04221473232765549\n",
      "epoch: 58, AP: 0.32436037677953383\n",
      "   2023-10-29 15:11:01.915368 batch 1 running loss 0.03810842614151777 loss 0.03810842614151777\n",
      "   2023-10-29 15:17:37.599278 batch 701 running loss 0.036491907689753356 loss 0.035213244752830275\n",
      "epoch: 59, AP: 0.3269343321695013\n",
      "   2023-10-29 15:18:28.822501 batch 1 running loss 0.035995140083176724 loss 0.035995140083176724\n",
      "   2023-10-29 15:25:05.217250 batch 701 running loss 0.04121621512176598 loss 0.03922439018624532\n",
      "epoch: 60, AP: 0.31948848558287296\n",
      "   2023-10-29 15:25:52.814342 batch 1 running loss 0.04171361973423149 loss 0.04171361973423149\n",
      "   2023-10-29 15:32:28.549960 batch 701 running loss 0.03831193941385051 loss 0.03381592579445869\n",
      "epoch: 61, AP: 0.31787443253956593\n",
      "   2023-10-29 15:33:21.016474 batch 1 running loss 0.036998814395266755 loss 0.036998814395266755\n",
      "   2023-10-29 15:39:56.415809 batch 701 running loss 0.03854262649211896 loss 0.04040572007339657\n",
      "epoch: 62, AP: 0.31569513061081966\n",
      "   2023-10-29 15:40:47.930351 batch 1 running loss 0.034201265357824195 loss 0.034201265357824195\n",
      "   2023-10-29 15:47:21.043868 batch 701 running loss 0.04127761720972336 loss 0.04604849655508336\n",
      "epoch: 63, AP: 0.3220043561934547\n",
      "   2023-10-29 15:48:12.139101 batch 1 running loss 0.043616600531030336 loss 0.043616600531030336\n",
      "   2023-10-29 15:54:48.830562 batch 701 running loss 0.03674551856477878 loss 0.03428940050569407\n",
      "epoch: 64, AP: 0.32235778232776324\n",
      "   2023-10-29 15:55:40.632239 batch 1 running loss 0.041557083437221976 loss 0.041557083437221976\n",
      "   2023-10-29 16:02:14.696717 batch 701 running loss 0.03688773021474002 loss 0.03502223124355483\n",
      "epoch: 65, AP: 0.32527752419730527\n",
      "   2023-10-29 16:03:03.730481 batch 1 running loss 0.03805087959003628 loss 0.03805087959003628\n",
      "   2023-10-29 16:09:38.635567 batch 701 running loss 0.03528228985252049 loss 0.03307883371370435\n",
      "epoch: 66, AP: 0.32615697928331056\n",
      "   2023-10-29 16:10:30.617502 batch 1 running loss 0.037103851939571386 loss 0.037103851939571386\n",
      "   2023-10-29 16:17:07.407893 batch 701 running loss 0.035455955972106226 loss 0.03861436715058966\n",
      "epoch: 67, AP: 0.32515688453667485\n",
      "   2023-10-29 16:17:58.817360 batch 1 running loss 0.03339210626398864 loss 0.03339210626398864\n",
      "   2023-10-29 16:24:32.114431 batch 701 running loss 0.037887226342946514 loss 0.036444986840152016\n",
      "epoch: 68, AP: 0.3269006631079723\n",
      "   2023-10-29 16:25:24.118597 batch 1 running loss 0.03254357607894531 loss 0.03254357607894531\n",
      "   2023-10-29 16:31:59.996773 batch 701 running loss 0.03524105328455928 loss 0.03291067678814738\n",
      "epoch: 69, AP: 0.32654164464166924\n",
      "   2023-10-29 16:32:51.616913 batch 1 running loss 0.033138768510820316 loss 0.033138768510820316\n",
      "   2023-10-29 16:39:27.714703 batch 701 running loss 0.03660193179109338 loss 0.03560061739910099\n",
      "epoch: 70, AP: 0.31946276462495576\n",
      "   2023-10-29 16:40:17.511508 batch 1 running loss 0.040390955001012974 loss 0.040390955001012974\n",
      "   2023-10-29 16:46:51.422356 batch 701 running loss 0.037372549951671066 loss 0.03367505284443817\n",
      "epoch: 71, AP: 0.3211577789167497\n",
      "   2023-10-29 16:47:42.115187 batch 1 running loss 0.039910945112230575 loss 0.039910945112230575\n",
      "   2023-10-29 16:54:17.714118 batch 701 running loss 0.03849633740110227 loss 0.03830109442911647\n",
      "epoch: 72, AP: 0.31931859086224423\n",
      "   2023-10-29 16:55:09.214676 batch 1 running loss 0.03431908559422908 loss 0.03431908559422908\n",
      "   2023-10-29 17:01:41.214462 batch 701 running loss 0.03562107602805435 loss 0.030970901997281086\n",
      "epoch: 73, AP: 0.31666186067973945\n",
      "   2023-10-29 17:02:32.017435 batch 1 running loss 0.03744578034883643 loss 0.03744578034883643\n",
      "   2023-10-29 17:09:07.215144 batch 701 running loss 0.03481992019797226 loss 0.033398902314491455\n",
      "epoch: 74, AP: 0.31677801376533643\n",
      "   2023-10-29 17:09:58.912401 batch 1 running loss 0.03480019666459644 loss 0.03480019666459644\n",
      "   2023-10-29 17:16:34.796785 batch 701 running loss 0.03630201024027075 loss 0.04029691472528357\n",
      "epoch: 75, AP: 0.3205488609771956\n",
      "   2023-10-29 17:17:25.354486 batch 1 running loss 0.03560126757676056 loss 0.03560126757676056\n",
      "   2023-10-29 17:23:59.616194 batch 701 running loss 0.035518348167356086 loss 0.033550277514079616\n",
      "epoch: 76, AP: 0.3257848495507616\n",
      "   2023-10-29 17:24:51.196625 batch 1 running loss 0.03512777538676559 loss 0.03512777538676559\n",
      "   2023-10-29 17:31:26.314507 batch 701 running loss 0.03464134974043435 loss 0.034213455111607394\n",
      "epoch: 77, AP: 0.3212833145404545\n",
      "   2023-10-29 17:32:17.847293 batch 1 running loss 0.031570082600537674 loss 0.031570082600537674\n",
      "   2023-10-29 17:38:50.813361 batch 701 running loss 0.032035494951298114 loss 0.029441322928856095\n",
      "epoch: 78, AP: 0.32626985984183166\n",
      "   2023-10-29 17:39:40.233479 batch 1 running loss 0.03627797494034879 loss 0.03627797494034879\n",
      "   2023-10-29 17:46:13.916240 batch 701 running loss 0.03449330834877156 loss 0.03536820716653827\n",
      "epoch: 79, AP: 0.32352963874719687\n",
      "   2023-10-29 17:47:06.215582 batch 1 running loss 0.03422471528419532 loss 0.03422471528419532\n",
      "   2023-10-29 17:53:42.936841 batch 701 running loss 0.03636709200571765 loss 0.0386035965749994\n",
      "epoch: 80, AP: 0.3140274829412494\n",
      "   2023-10-29 17:54:34.550701 batch 1 running loss 0.031250917747098816 loss 0.031250917747098816\n",
      "   2023-10-29 18:01:04.520067 batch 701 running loss 0.03455378777533243 loss 0.032245918349503466\n",
      "epoch: 81, AP: 0.3127786730328459\n",
      "   2023-10-29 18:01:55.496750 batch 1 running loss 0.04018950352305299 loss 0.04018950352305299\n",
      "   2023-10-29 18:08:30.412318 batch 701 running loss 0.036266060285863815 loss 0.03522167025887579\n",
      "epoch: 82, AP: 0.3150724119212792\n",
      "   2023-10-29 18:09:21.222141 batch 1 running loss 0.03775967724370638 loss 0.03775967724370638\n",
      "   2023-10-29 18:15:58.524197 batch 701 running loss 0.03585577749517626 loss 0.03532498601515085\n",
      "epoch: 83, AP: 0.31887031998232107\n",
      "   2023-10-29 18:16:46.818268 batch 1 running loss 0.03310533170729882 loss 0.03310533170729882\n",
      "   2023-10-29 18:23:21.124905 batch 701 running loss 0.03438487542406726 loss 0.0327819230414\n",
      "epoch: 84, AP: 0.3171411695719494\n",
      "   2023-10-29 18:24:12.918728 batch 1 running loss 0.03298174404865539 loss 0.03298174404865539\n",
      "   2023-10-29 18:30:47.696629 batch 701 running loss 0.03319043064948274 loss 0.03469768596032646\n",
      "epoch: 85, AP: 0.31747655534281866\n",
      "   2023-10-29 18:31:38.526404 batch 1 running loss 0.033144138253634575 loss 0.033144138253634575\n",
      "   2023-10-29 18:38:12.024968 batch 701 running loss 0.03242589005114558 loss 0.0288228360284572\n",
      "epoch: 86, AP: 0.31853866296750527\n",
      "   2023-10-29 18:39:03.418151 batch 1 running loss 0.03328844068417716 loss 0.03328844068417716\n",
      "   2023-10-29 18:45:42.319617 batch 701 running loss 0.032354047205343704 loss 0.03160791539352473\n",
      "epoch: 87, AP: 0.3205276089036144\n",
      "   2023-10-29 18:46:33.815279 batch 1 running loss 0.02573058644683035 loss 0.02573058644683035\n",
      "   2023-10-29 18:53:09.815816 batch 701 running loss 0.03250350898334369 loss 0.033932187401273274\n",
      "epoch: 88, AP: 0.3242948998766525\n",
      "   2023-10-29 18:53:58.043083 batch 1 running loss 0.031128852457810732 loss 0.031128852457810732\n",
      "   2023-10-29 19:00:37.146320 batch 701 running loss 0.031098526949737602 loss 0.03363636266795031\n",
      "epoch: 89, AP: 0.322328458078051\n",
      "   2023-10-29 19:01:28.718595 batch 1 running loss 0.033636557921586416 loss 0.033636557921586416\n",
      "   2023-10-29 19:08:03.218368 batch 701 running loss 0.03623488509502585 loss 0.041538601915817014\n",
      "epoch: 90, AP: 0.3125306491458705\n",
      "   2023-10-29 19:08:53.917386 batch 1 running loss 0.030862975081350275 loss 0.030862975081350275\n",
      "   2023-10-29 19:15:26.696811 batch 701 running loss 0.0334784422803342 loss 0.031239682644173337\n",
      "epoch: 91, AP: 0.31414977578217085\n",
      "   2023-10-29 19:16:18.221250 batch 1 running loss 0.031845586006132826 loss 0.031845586006132826\n",
      "   2023-10-29 19:22:53.408983 batch 701 running loss 0.03212713624062928 loss 0.03184710055599953\n",
      "epoch: 92, AP: 0.31172395780683937\n",
      "   2023-10-29 19:23:44.817279 batch 1 running loss 0.03464114554939451 loss 0.03464114554939451\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "from timm.loss import AsymmetricLossMultiLabel\n",
    "from transformers import set_seed\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "\n",
    "model = Network()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "epochs = 150\n",
    "model = model.to(CUDA_DEV)\n",
    "criterion = criterion.to(CUDA_DEV)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, eta_min=1e-7)\n",
    "\n",
    "best = 0\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_epoch(model, train_dataloader, criterion, optimizer, scheduler, print_loss=True, iteration_step=700, epoch=epoch)\n",
    "    track_idxs, predictions, targets = predict_train(model, val_dataloader)\n",
    "    ap = sklearn.metrics.average_precision_score(targets, predictions)\n",
    "    print(f\"epoch: {epoch}, AP: {ap}\")\n",
    "    if (ap > best):\n",
    "        best = ap\n",
    "        best_model = model\n",
    "        if (epoch > 20):\n",
    "            track_idxs, predictions = predict(model, test_dataloader)\n",
    "            for i, c in enumerate(predictions.argmax(-1)):\n",
    "                probs = np.array([1 + dict_tags[c].get(t, 0) for t in np.arange(predictions.shape[1])])\n",
    "                probs[c] = 2\n",
    "                predictions[i] = predictions[i] * probs\n",
    "                predictions[i] /= predictions[i].sum()\n",
    "                        \n",
    "            predictions_df = pd.DataFrame([\n",
    "                {'track': track, 'prediction': ','.join([str(p) for p in probs])}\n",
    "                for track, probs in zip(track_idxs, predictions)\n",
    "            ])\n",
    "            predictions_df.to_csv(f'subs/prediction_best_100.csv', index=False)\n",
    "            torch.save(best_model.state_dict(), f'models/models_best_100.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa898f51-c075-47fc-95a0-b4f95ed655b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP: 0.3199148484354201\n",
      "POST AP: 0.32061250757510396\n",
      "POST-1 AP: 0.3215476797234166\n"
     ]
    }
   ],
   "source": [
    "track_idxs, predictions, targets = predict_train(best_model, val_dataloader)\n",
    "ap = sklearn.metrics.average_precision_score(targets, predictions)\n",
    "print(f\"AP: {ap}\")\n",
    "\n",
    "for i, c in enumerate(predictions.argmax(-1)):\n",
    "    probs = np.array([1 + dict_tags[c].get(t, 0) for t in np.arange(predictions.shape[1])])\n",
    "    probs[c] = 2\n",
    "    predictions[i] = predictions[i] * probs\n",
    "    predictions[i] /= predictions[i].sum()\n",
    "ap = sklearn.metrics.average_precision_score(targets, predictions)\n",
    "\n",
    "print(f\"POST AP: {ap}\")\n",
    "\n",
    "for i, c in enumerate(predictions.argmax(-1)):\n",
    "    probs = np.array([1 + dict_tags[c].get(t, 0) for t in np.arange(predictions.shape[1])])\n",
    "    probs[c] = 2\n",
    "    predictions[i] = predictions[i] * probs\n",
    "    predictions[i] /= predictions[i].sum()\n",
    "ap = sklearn.metrics.average_precision_score(targets, predictions)\n",
    "\n",
    "print(f\"POST-1 AP: {ap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4daf63c5-45b4-4cb8-99f1-534fc8203caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_idxs, predictions = predict(best_model, test_dataloader)\n",
    "\n",
    "for i, c in enumerate(predictions.argmax(-1)):\n",
    "    probs = np.array([1 + dict_tags[c].get(t, 0) for t in np.arange(predictions.shape[1])])\n",
    "    probs[c] = 2\n",
    "    predictions[i] = predictions[i] * probs\n",
    "    predictions[i] /= predictions[i].sum()\n",
    "    \n",
    "for i, c in enumerate(predictions.argmax(-1)):\n",
    "    probs = np.array([1 + dict_tags[c].get(t, 0) for t in np.arange(predictions.shape[1])])\n",
    "    probs[c] = 2\n",
    "    predictions[i] = predictions[i] * probs\n",
    "    predictions[i] /= predictions[i].sum()\n",
    "\n",
    "predictions_df = pd.DataFrame([\n",
    "    {'track': track, 'prediction': ','.join([str(p) for p in probs])}\n",
    "    for track, probs in zip(track_idxs, predictions)\n",
    "])\n",
    "predictions_df.to_csv(f'prediction_best_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ae520e-08d1-4a77-85e3-d0023c8fc8a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ya",
   "language": "python",
   "name": "ya"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
