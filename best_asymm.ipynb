{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "340aceb6-4360-49ac-8381-bb42055d6146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.remove('/home/jovyan/.imgenv-lm-poly-0/lib/python3.7/site-packages')\n",
    "os.environ['PYTHONPATH'] = '/home/user/conda/envs/ya/lib/python3.10/site-packages'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32801790-6619-4141-b909-a70de5e00071",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_DEV = 0\n",
    "NUM_TAGS = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29f35090-eb3d-464b-b46f-de30c4d42d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "144deed2-ca15-41aa-9225-6edf01d1003e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "tags = [[int(i) for i in x.split(',')] for x in df_train.tags.values]\n",
    "dict_tags = {}\n",
    "for cls_tags in tags:\n",
    "    for c in cls_tags:\n",
    "        if c not in dict_tags.keys():\n",
    "            dict_tags[c] = Counter(cls_tags)\n",
    "        else:\n",
    "            dict_tags[c].update(Counter(cls_tags))\n",
    "            \n",
    "for tag in dict_tags.keys():\n",
    "    del dict_tags[tag][tag]\n",
    "    n = np.sum(list(dict_tags[tag].values()))\n",
    "    for t in dict_tags[tag].keys():\n",
    "        dict_tags[tag][t] = dict_tags[tag][t]/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b46b7d4b-785e-4187-b2bc-6c620c7b8cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 76715/76715 [03:40<00:00, 347.45it/s]\n"
     ]
    }
   ],
   "source": [
    "track_idx2embeds = {}\n",
    "for fn in tqdm(glob('track_embeddings/*')):\n",
    "    name = fn.split('/')[1].split('.')[0]\n",
    "    if name == \"track_embeddings\":\n",
    "        continue\n",
    "    track_idx = int(name)\n",
    "    embeds = np.load(fn)\n",
    "    track_idx2embeds[track_idx] = embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d1b5cae-60ae-4584-a6bb-4f6b833929aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaggingDataset(Dataset):\n",
    "    def __init__(self, df, aug=0, testing=False):\n",
    "        self.df = df\n",
    "        self.testing = testing\n",
    "        self.aug = aug\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        track_idx = row.track\n",
    "        embeds = track_idx2embeds[track_idx]\n",
    "        if self.testing:\n",
    "            return track_idx, embeds\n",
    "        tags = [int(x) for x in row.tags.split(',')]\n",
    "        target = np.zeros(NUM_TAGS)\n",
    "        target[tags] = 1\n",
    "        \n",
    "        if np.random.choice([0, 1], p=[1 - self.aug, self.aug]):\n",
    "            s = np.random.uniform(0.0, 0.4)\n",
    "            e = np.random.uniform(s+0.1, 1)\n",
    "            s = int(s * embeds.shape[0])\n",
    "            e = int(e * embeds.shape[0])\n",
    "            embeds = embeds[s:e]\n",
    "        \n",
    "        return track_idx, embeds, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca9ac5cf-a481-4918-bbeb-ecf077c681ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TaggingDataset(df_train[:-1000], aug=0.6)\n",
    "val_dataset = TaggingDataset(df_train[-1000:])\n",
    "\n",
    "test_dataset = TaggingDataset(df_test, testing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31c659b7-ee4b-44da-a715-b7abced07279",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, emb_dim=768, mult=4, p=0.0):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(emb_dim, emb_dim * mult),\n",
    "            nn.Dropout(p),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(emb_dim * mult, emb_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "    \n",
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Sequential(\n",
    "            nn.Linear(embedding_size, embedding_size),\n",
    "            nn.LayerNorm(embedding_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embedding_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_logits = self.attn(x)\n",
    "        if mask is not None:\n",
    "            attn_logits[mask] = -float('inf')\n",
    "        attn_weights = torch.softmax(attn_logits, dim=1)\n",
    "        x = x * attn_weights\n",
    "        x = x.sum(dim=1)\n",
    "        return x\n",
    "    \n",
    "class Network(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes = NUM_TAGS,\n",
    "        input_dim = 768,\n",
    "        hidden_dim = 512\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.position_enc = nn.Embedding(128, input_dim, padding_idx=-1) \n",
    "        self.proj = FeedForward(input_dim)\n",
    "        self.bn = nn.BatchNorm1d(input_dim)\n",
    "        self.ln = nn.LayerNorm(input_dim)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=768, nhead=12, activation=\"gelu\", batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=6)\n",
    "        self.poooling = AttentionPooling(input_dim)\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "               \n",
    "    def forward(self, embeds):\n",
    "        embeds = self.proj(embeds)\n",
    "        src_key_padding_mask = (embeds.mean(-1) == -1)\n",
    "        embeds = self.ln(embeds)\n",
    "        x = self.transformer_encoder(embeds, src_key_padding_mask=src_key_padding_mask)\n",
    "        x = self.bn(self.poooling(x, mask=src_key_padding_mask))\n",
    "        outs = self.fc(x)\n",
    "        return outs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c70bf034-7966-4f44-9f2e-dcaf0f8a8184",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, scheduler, print_loss=True, iteration_step=100, epoch=0):\n",
    "    model.train()\n",
    "    running_loss = None\n",
    "    alpha = 0.8\n",
    "    iters = len(loader)\n",
    "    for iteration,data in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        track_idxs, embeds, target = data\n",
    "        embeds = [x.to(CUDA_DEV) for x in embeds]\n",
    "        embeds = pad_sequence(embeds, padding_value=-1, batch_first=True)[:, :64, :]\n",
    "        target = target.to(CUDA_DEV)\n",
    "        pred_logits = model(embeds)\n",
    "        pred_probs = torch.sigmoid(pred_logits)\n",
    "        ce_loss = criterion(pred_logits, target)\n",
    "            \n",
    "        ce_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        scheduler.step(epoch + iteration / iters)\n",
    "        \n",
    "        if running_loss is None:\n",
    "            running_loss = ce_loss.item()\n",
    "        else:\n",
    "            running_loss = alpha * running_loss + (1 - alpha) * ce_loss.item()\n",
    "        if (iteration % iteration_step == 0) and print_loss:\n",
    "            print('   {} batch {} running loss {} loss {}'.format(\n",
    "                datetime.now(), iteration + 1, running_loss, ce_loss.item()\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e624b0b-8daf-4702-a5de-c667fcd53121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, loader):\n",
    "    model.eval()\n",
    "    track_idxs = []\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            track_idx, embeds = data\n",
    "            embeds = [x.to(CUDA_DEV) for x in embeds]\n",
    "            embeds = pad_sequence(embeds, padding_value=-1, batch_first=True)[:, :64, :]\n",
    "            pred_logits = model(embeds)\n",
    "            pred_probs = torch.sigmoid(pred_logits)\n",
    "            predictions.append(pred_probs.cpu().numpy())\n",
    "            track_idxs.append(track_idx.numpy())\n",
    "    predictions = np.vstack(predictions)\n",
    "    track_idxs = np.vstack(track_idxs).ravel()\n",
    "    return track_idxs, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37c01c5a-6d40-47d3-a535-d3349b41139e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def predict_train(model, loader):\n",
    "    model.eval()\n",
    "    track_idxs = []\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            track_idx, embeds, target = data\n",
    "            embeds = [x.to(CUDA_DEV) for x in embeds]\n",
    "            embeds = pad_sequence(embeds, padding_value=-1, batch_first=True)[:, :64, :]\n",
    "            pred_logits = model(embeds)\n",
    "            pred_probs = torch.sigmoid(pred_logits)\n",
    "            predictions.append(pred_probs.cpu().numpy())\n",
    "            track_idxs.append(track_idx.numpy())\n",
    "            targets.append(target.numpy())\n",
    "    predictions = np.vstack(predictions)\n",
    "    targets = np.vstack(targets)\n",
    "    track_idxs = np.vstack(track_idxs).ravel()\n",
    "    return track_idxs, predictions, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4decde5-005a-4820-804c-cf4d110c799e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(b):\n",
    "    track_idxs = torch.from_numpy(np.vstack([x[0] for x in b]))\n",
    "    embeds = [torch.from_numpy(x[1]) for x in b]\n",
    "    targets = np.vstack([x[2] for x in b])\n",
    "    targets = torch.from_numpy(targets)\n",
    "    return track_idxs, embeds, targets\n",
    "\n",
    "def collate_fn_test(b):\n",
    "    track_idxs = torch.from_numpy(np.vstack([x[0] for x in b]))\n",
    "    embeds = [torch.from_numpy(x[1]) for x in b]\n",
    "    return track_idxs, embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62124a85-3bcc-4eb8-a0d7-931c8685d1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=128, shuffle=False, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False, collate_fn=collate_fn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448ff0ee-2a61-4761-8d54-8b555bca7a09",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/conda/envs/ya/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2023-10-29 09:08:04.264839 batch 1 running loss 1002.988423173563 loss 1002.988423173563\n",
      "   2023-10-29 09:09:20.719416 batch 701 running loss 455.08679623700954 loss 459.08829876718056\n",
      "epoch: 0, AP: 0.11052579929268819\n",
      "   2023-10-29 09:09:30.735355 batch 1 running loss 455.7109616683606 loss 455.7109616683606\n",
      "   2023-10-29 09:11:21.883372 batch 701 running loss 419.33413650339816 loss 423.27108874283294\n",
      "epoch: 1, AP: 0.14605386089891065\n",
      "   2023-10-29 09:11:42.896712 batch 1 running loss 417.5027214490381 loss 417.5027214490381\n",
      "   2023-10-29 09:14:23.955382 batch 701 running loss 374.1607435659656 loss 375.13316946142146\n",
      "epoch: 2, AP: 0.17554764233471146\n",
      "   2023-10-29 09:14:44.908447 batch 1 running loss 368.0893333015814 loss 368.0893333015814\n",
      "   2023-10-29 09:17:27.544295 batch 701 running loss 322.7401039000439 loss 326.188013943049\n",
      "epoch: 3, AP: 0.19768742796246278\n",
      "   2023-10-29 09:17:48.460764 batch 1 running loss 331.30668007976243 loss 331.30668007976243\n",
      "   2023-10-29 09:20:31.465017 batch 701 running loss 285.0762023841865 loss 279.87998971647784\n",
      "epoch: 4, AP: 0.2106031778193142\n",
      "   2023-10-29 09:20:52.555551 batch 1 running loss 287.66113419528756 loss 287.66113419528756\n",
      "   2023-10-29 09:23:35.467314 batch 701 running loss 263.2882847452794 loss 260.2571158834497\n",
      "epoch: 5, AP: 0.2283282844639764\n",
      "   2023-10-29 09:23:56.531744 batch 1 running loss 259.31981173577014 loss 259.31981173577014\n",
      "   2023-10-29 09:26:39.252336 batch 701 running loss 245.9568951139195 loss 245.3887496259602\n",
      "epoch: 6, AP: 0.2383777822259895\n",
      "   2023-10-29 09:27:00.264995 batch 1 running loss 244.2475232288703 loss 244.2475232288703\n",
      "   2023-10-29 09:29:43.217251 batch 701 running loss 236.80759010165121 loss 241.35126500999064\n",
      "epoch: 7, AP: 0.24033780310267416\n",
      "   2023-10-29 09:30:03.716199 batch 1 running loss 239.0718263175964 loss 239.0718263175964\n",
      "   2023-10-29 09:32:47.325355 batch 701 running loss 240.04092131521878 loss 254.91012417945996\n",
      "epoch: 8, AP: 0.24480980792757104\n",
      "   2023-10-29 09:33:07.997201 batch 1 running loss 225.31490462968077 loss 225.31490462968077\n",
      "   2023-10-29 09:35:50.555206 batch 701 running loss 234.96469450242304 loss 238.81429720471303\n",
      "epoch: 9, AP: 0.2429806784926923\n",
      "   2023-10-29 09:36:11.697487 batch 1 running loss 241.56450406603315 loss 241.56450406603315\n",
      "   2023-10-29 09:38:53.915753 batch 701 running loss 206.4743372694802 loss 208.0665487555531\n",
      "epoch: 10, AP: 0.23403956368932027\n",
      "   2023-10-29 09:39:14.967766 batch 1 running loss 201.14260857313815 loss 201.14260857313815\n",
      "   2023-10-29 09:41:56.917189 batch 701 running loss 174.20504349236768 loss 179.3431608912215\n",
      "epoch: 11, AP: 0.25113841794354475\n",
      "   2023-10-29 09:42:17.798570 batch 1 running loss 180.52307144516635 loss 180.52307144516635\n",
      "   2023-10-29 09:45:01.283473 batch 701 running loss 158.9316354154544 loss 164.17800729687508\n",
      "epoch: 12, AP: 0.26427021567929526\n",
      "   2023-10-29 09:45:22.199332 batch 1 running loss 154.61561111208644 loss 154.61561111208644\n",
      "   2023-10-29 09:48:05.311460 batch 701 running loss 151.39711647416087 loss 150.61268154345555\n",
      "epoch: 13, AP: 0.2782937081467448\n",
      "   2023-10-29 09:48:26.459440 batch 1 running loss 141.27063352049285 loss 141.27063352049285\n",
      "   2023-10-29 09:51:12.087567 batch 701 running loss 143.25844126075987 loss 146.63960708133624\n",
      "epoch: 14, AP: 0.27974274103665575\n",
      "   2023-10-29 09:51:32.862115 batch 1 running loss 136.23861662545113 loss 136.23861662545113\n",
      "   2023-10-29 09:54:16.332148 batch 701 running loss 142.7905305235158 loss 145.1504074103762\n",
      "epoch: 15, AP: 0.2910181884083159\n",
      "   2023-10-29 09:54:37.477544 batch 1 running loss 135.66348578940028 loss 135.66348578940028\n",
      "   2023-10-29 09:57:21.438992 batch 701 running loss 140.24224616993675 loss 129.56249743105016\n",
      "epoch: 16, AP: 0.295934648244898\n",
      "   2023-10-29 09:57:42.296781 batch 1 running loss 131.45086918383694 loss 131.45086918383694\n",
      "   2023-10-29 10:00:27.106006 batch 701 running loss 135.980168889079 loss 129.5265133690386\n",
      "epoch: 17, AP: 0.3002269743177692\n",
      "   2023-10-29 10:00:48.083481 batch 1 running loss 137.01615149139127 loss 137.01615149139127\n",
      "   2023-10-29 10:03:32.186912 batch 701 running loss 139.2472656901376 loss 138.6374551716092\n",
      "epoch: 18, AP: 0.3005049370606391\n",
      "   2023-10-29 10:03:53.218608 batch 1 running loss 133.54744287197508 loss 133.54744287197508\n",
      "   2023-10-29 10:06:37.739877 batch 701 running loss 133.78659361679905 loss 124.5492903962994\n",
      "epoch: 19, AP: 0.30201676969403113\n",
      "   2023-10-29 10:06:58.925114 batch 1 running loss 127.92736619853702 loss 127.92736619853702\n",
      "   2023-10-29 10:09:44.016827 batch 701 running loss 134.6233389409306 loss 138.9878021311457\n",
      "epoch: 20, AP: 0.2864514600934959\n",
      "   2023-10-29 10:10:05.243264 batch 1 running loss 126.46938869201762 loss 126.46938869201762\n",
      "   2023-10-29 10:12:51.603915 batch 701 running loss 136.17586090962848 loss 143.60946209091838\n",
      "epoch: 21, AP: 0.29003612962641667\n",
      "   2023-10-29 10:13:12.096663 batch 1 running loss 141.55096438851865 loss 141.55096438851865\n",
      "   2023-10-29 10:16:30.049404 batch 701 running loss 124.95796692854404 loss 118.76952595932775\n",
      "epoch: 22, AP: 0.3021570097784181\n",
      "   2023-10-29 10:18:17.352338 batch 1 running loss 124.31148624350082 loss 124.31148624350082\n",
      "   2023-10-29 10:21:20.083250 batch 701 running loss 132.89502532043736 loss 137.55553050857952\n",
      "epoch: 23, AP: 0.3067896535260727\n",
      "   2023-10-29 10:22:42.562574 batch 1 running loss 132.21026570836239 loss 132.21026570836239\n",
      "   2023-10-29 10:26:41.596807 batch 701 running loss 123.6299924164121 loss 117.16421351370897\n",
      "epoch: 24, AP: 0.3153838298554507\n",
      "   2023-10-29 10:27:49.626683 batch 1 running loss 110.56927129386607 loss 110.56927129386607\n",
      "   2023-10-29 10:30:32.774038 batch 701 running loss 125.82775241314349 loss 127.19937074359756\n",
      "epoch: 25, AP: 0.3177142638103275\n",
      "   2023-10-29 10:31:39.036400 batch 1 running loss 115.1190869647572 loss 115.1190869647572\n",
      "   2023-10-29 10:34:25.540145 batch 701 running loss 119.70852129221224 loss 123.35908287251829\n",
      "epoch: 26, AP: 0.31738997634856336\n",
      "   2023-10-29 10:34:47.152648 batch 1 running loss 118.89847121738526 loss 118.89847121738526\n",
      "   2023-10-29 10:38:39.772093 batch 701 running loss 122.05295902194959 loss 124.092292418438\n",
      "epoch: 27, AP: 0.3204014482880866\n",
      "   2023-10-29 10:40:21.096698 batch 1 running loss 117.91133583333098 loss 117.91133583333098\n",
      "   2023-10-29 10:43:04.628058 batch 701 running loss 116.89826971673276 loss 115.43994087029937\n",
      "epoch: 28, AP: 0.3214149244524946\n",
      "   2023-10-29 10:44:09.018595 batch 1 running loss 123.95246740066575 loss 123.95246740066575\n",
      "   2023-10-29 10:46:44.986365 batch 701 running loss 118.03342356674631 loss 130.0770894973486\n",
      "epoch: 29, AP: 0.3230843590349296\n",
      "   2023-10-29 10:47:52.879938 batch 1 running loss 126.61933203206438 loss 126.61933203206438\n",
      "   2023-10-29 10:50:12.668148 batch 701 running loss 129.62332876290395 loss 132.0569377490969\n",
      "epoch: 30, AP: 0.3020806938721079\n",
      "   2023-10-29 10:50:34.265732 batch 1 running loss 125.93816896172882 loss 125.93816896172882\n",
      "   2023-10-29 10:53:57.817115 batch 701 running loss 122.99792665785975 loss 145.08967184715175\n",
      "epoch: 31, AP: 0.3057602024793225\n",
      "   2023-10-29 10:54:20.811591 batch 1 running loss 122.35560668359797 loss 122.35560668359797\n",
      "   2023-10-29 10:57:45.608405 batch 701 running loss 118.32209521750215 loss 128.87648287833025\n",
      "epoch: 32, AP: 0.3197843023254241\n",
      "   2023-10-29 10:58:06.735264 batch 1 running loss 113.52552731940742 loss 113.52552731940742\n",
      "   2023-10-29 11:00:37.396720 batch 701 running loss 121.58814571828435 loss 131.7689186203271\n",
      "epoch: 33, AP: 0.3120843516621049\n",
      "   2023-10-29 11:01:01.514398 batch 1 running loss 106.25016981447914 loss 106.25016981447914\n",
      "   2023-10-29 11:04:14.256852 batch 701 running loss 118.89975135905316 loss 125.7387320669576\n",
      "epoch: 34, AP: 0.3264730978539114\n",
      "   2023-10-29 11:05:23.831325 batch 1 running loss 132.39560958402774 loss 132.39560958402774\n",
      "   2023-10-29 11:08:37.067965 batch 701 running loss 112.64728289087833 loss 103.94152335169798\n",
      "epoch: 35, AP: 0.3189767724300281\n",
      "   2023-10-29 11:09:01.446902 batch 1 running loss 100.28583776146743 loss 100.28583776146743\n",
      "   2023-10-29 11:11:58.896724 batch 701 running loss 115.55714801499664 loss 115.93862561022875\n",
      "epoch: 36, AP: 0.32836044948906684\n",
      "   2023-10-29 11:13:11.826419 batch 1 running loss 124.63075205460095 loss 124.63075205460095\n",
      "   2023-10-29 11:15:28.696748 batch 701 running loss 111.61598944527263 loss 109.28298819108502\n",
      "epoch: 37, AP: 0.33072636764138746\n",
      "   2023-10-29 11:16:39.180759 batch 1 running loss 109.4468405820778 loss 109.4468405820778\n",
      "   2023-10-29 11:20:16.042098 batch 701 running loss 109.34699193174167 loss 97.49193856018226\n",
      "epoch: 38, AP: 0.3296772369681572\n",
      "   2023-10-29 11:20:48.922924 batch 1 running loss 114.77319174737255 loss 114.77319174737255\n",
      "   2023-10-29 11:26:02.311011 batch 701 running loss 112.67737404726849 loss 111.61281737173617\n",
      "epoch: 39, AP: 0.33029356878909233\n",
      "   2023-10-29 11:26:42.730383 batch 1 running loss 107.50445666327218 loss 107.50445666327218\n",
      "   2023-10-29 11:31:55.628230 batch 701 running loss 124.43882406784084 loss 132.291969202421\n",
      "epoch: 40, AP: 0.3178350541647068\n",
      "   2023-10-29 11:32:36.814707 batch 1 running loss 117.67113198890443 loss 117.67113198890443\n",
      "   2023-10-29 11:37:49.516389 batch 701 running loss 114.54902349873535 loss 117.01994997626205\n",
      "epoch: 41, AP: 0.32288532297647377\n",
      "   2023-10-29 11:38:30.496660 batch 1 running loss 118.05186592001756 loss 118.05186592001756\n",
      "   2023-10-29 11:43:42.797136 batch 701 running loss 116.4261476684992 loss 102.15907637624012\n",
      "epoch: 42, AP: 0.3217940756658047\n",
      "   2023-10-29 11:44:24.696646 batch 1 running loss 126.14175902952226 loss 126.14175902952226\n",
      "   2023-10-29 11:49:36.736118 batch 701 running loss 112.29281701925075 loss 103.85264322943738\n",
      "epoch: 43, AP: 0.327857890382596\n",
      "   2023-10-29 11:50:17.396696 batch 1 running loss 118.10578504892867 loss 118.10578504892867\n",
      "   2023-10-29 11:55:30.338008 batch 701 running loss 116.45789192266865 loss 116.4001968375054\n",
      "epoch: 44, AP: 0.328426056110492\n",
      "   2023-10-29 11:56:11.513790 batch 1 running loss 130.15411592185984 loss 130.15411592185984\n",
      "   2023-10-29 12:01:04.056155 batch 701 running loss 111.86971538119315 loss 110.01086273588993\n",
      "epoch: 45, AP: 0.32690322482611345\n",
      "   2023-10-29 12:01:45.144904 batch 1 running loss 97.59215071738933 loss 97.59215071738933\n",
      "   2023-10-29 12:06:58.336021 batch 701 running loss 110.55947772801183 loss 108.68205097012401\n",
      "epoch: 46, AP: 0.3317555349753984\n",
      "   2023-10-29 12:09:08.130801 batch 1 running loss 112.85418619348197 loss 112.85418619348197\n",
      "   2023-10-29 12:14:21.097294 batch 701 running loss 111.42947294669366 loss 125.39507869722412\n",
      "epoch: 47, AP: 0.3313727932117525\n",
      "   2023-10-29 12:15:02.296732 batch 1 running loss 111.33064992210183 loss 111.33064992210183\n",
      "   2023-10-29 12:19:55.513434 batch 701 running loss 114.26221901955533 loss 127.66827923390488\n",
      "epoch: 48, AP: 0.3294368752351827\n",
      "   2023-10-29 12:20:36.396645 batch 1 running loss 121.22693308939752 loss 121.22693308939752\n",
      "   2023-10-29 12:25:47.055094 batch 701 running loss 108.30232242584023 loss 96.37477434852411\n",
      "epoch: 49, AP: 0.32947378607107325\n",
      "   2023-10-29 12:26:25.115514 batch 1 running loss 113.95163397282998 loss 113.95163397282998\n",
      "   2023-10-29 12:31:21.016512 batch 701 running loss 111.7666104835569 loss 109.59348438032897\n",
      "epoch: 50, AP: 0.3177768425466397\n",
      "   2023-10-29 12:32:02.296589 batch 1 running loss 113.49171735063359 loss 113.49171735063359\n",
      "   2023-10-29 12:37:15.207275 batch 701 running loss 118.544443386636 loss 121.96484766059753\n",
      "epoch: 51, AP: 0.3208372439104875\n",
      "   2023-10-29 12:37:55.596727 batch 1 running loss 106.82947112231666 loss 106.82947112231666\n",
      "   2023-10-29 12:43:04.918413 batch 701 running loss 109.64176708221719 loss 108.51596468156316\n",
      "epoch: 52, AP: 0.3269520472498314\n",
      "   2023-10-29 12:43:40.840438 batch 1 running loss 108.8519839741623 loss 108.8519839741623\n",
      "   2023-10-29 12:48:42.128113 batch 701 running loss 109.25566915296764 loss 102.4212592964189\n",
      "epoch: 53, AP: 0.31889087234736857\n",
      "   2023-10-29 12:49:21.111030 batch 1 running loss 92.57683917476166 loss 92.57683917476166\n",
      "   2023-10-29 12:54:33.522656 batch 701 running loss 109.7965952675287 loss 114.3414833470258\n",
      "epoch: 54, AP: 0.31850836164553914\n",
      "   2023-10-29 12:55:14.060533 batch 1 running loss 126.94470397684425 loss 126.94470397684425\n",
      "   2023-10-29 13:00:25.100105 batch 701 running loss 107.00173537985313 loss 108.27317483397712\n",
      "epoch: 55, AP: 0.32854432514522497\n",
      "   2023-10-29 13:01:06.425897 batch 1 running loss 107.46154285978622 loss 107.46154285978622\n",
      "   2023-10-29 13:06:19.052527 batch 701 running loss 102.75696611058916 loss 97.17201278812729\n",
      "epoch: 56, AP: 0.3278112744038132\n",
      "   2023-10-29 13:07:00.116528 batch 1 running loss 99.61706265001064 loss 99.61706265001064\n",
      "   2023-10-29 13:12:14.032297 batch 701 running loss 105.25301059038758 loss 96.35795302522725\n",
      "epoch: 57, AP: 0.32667894893952537\n",
      "   2023-10-29 13:12:54.714772 batch 1 running loss 103.78501340596489 loss 103.78501340596489\n",
      "   2023-10-29 13:18:06.809318 batch 701 running loss 103.61221094628608 loss 96.9668177386824\n",
      "epoch: 58, AP: 0.3274687111589287\n",
      "   2023-10-29 13:18:48.015178 batch 1 running loss 98.0973567466811 loss 98.0973567466811\n",
      "   2023-10-29 13:23:59.513806 batch 701 running loss 97.59167148625998 loss 94.57584445588553\n",
      "epoch: 59, AP: 0.3276033075311904\n",
      "   2023-10-29 13:24:40.417235 batch 1 running loss 104.61472901957859 loss 104.61472901957859\n",
      "   2023-10-29 13:29:52.218397 batch 701 running loss 114.52334277717577 loss 108.94242535072098\n",
      "epoch: 60, AP: 0.32170104474325895\n",
      "   2023-10-29 13:30:33.148005 batch 1 running loss 105.36045200731006 loss 105.36045200731006\n",
      "   2023-10-29 13:35:47.552679 batch 701 running loss 103.60343125521098 loss 100.3495433793569\n",
      "epoch: 61, AP: 0.3232259414703047\n",
      "   2023-10-29 13:36:27.938817 batch 1 running loss 98.88635266524977 loss 98.88635266524977\n",
      "   2023-10-29 13:41:22.947253 batch 701 running loss 111.37873095482422 loss 106.78863895476923\n",
      "epoch: 62, AP: 0.32194268628937894\n",
      "   2023-10-29 13:42:02.104709 batch 1 running loss 106.39509241555425 loss 106.39509241555425\n",
      "   2023-10-29 13:47:04.807016 batch 701 running loss 109.0398501383221 loss 116.23106530047383\n",
      "epoch: 63, AP: 0.3229733958713148\n",
      "   2023-10-29 13:47:34.896775 batch 1 running loss 97.76230792854813 loss 97.76230792854813\n",
      "   2023-10-29 13:52:06.659056 batch 701 running loss 102.69687830830944 loss 100.25202955198645\n",
      "epoch: 64, AP: 0.3229032626080116\n",
      "   2023-10-29 13:52:37.415827 batch 1 running loss 102.89062128897316 loss 102.89062128897316\n",
      "   2023-10-29 13:56:35.820902 batch 701 running loss 104.17883279864542 loss 101.78601915293797\n",
      "epoch: 65, AP: 0.32530264514520646\n",
      "   2023-10-29 13:57:17.236150 batch 1 running loss 113.37509234433405 loss 113.37509234433405\n",
      "   2023-10-29 14:02:28.711369 batch 701 running loss 99.38645593550402 loss 92.65575913448332\n",
      "epoch: 66, AP: 0.3260797951920618\n",
      "   2023-10-29 14:03:09.541839 batch 1 running loss 91.04692045110363 loss 91.04692045110363\n",
      "   2023-10-29 14:08:21.810330 batch 701 running loss 101.78242576686661 loss 96.30494665468777\n",
      "epoch: 67, AP: 0.33055564117050146\n",
      "   2023-10-29 14:09:02.222115 batch 1 running loss 105.29147401040743 loss 105.29147401040743\n",
      "   2023-10-29 14:14:15.817069 batch 701 running loss 99.51080329747163 loss 90.85926086217937\n",
      "epoch: 68, AP: 0.3273619475712116\n",
      "   2023-10-29 14:14:56.331159 batch 1 running loss 105.97411856115544 loss 105.97411856115544\n",
      "   2023-10-29 14:20:09.718337 batch 701 running loss 97.5162506537945 loss 110.50129218476624\n",
      "epoch: 69, AP: 0.3273641258356309\n",
      "   2023-10-29 14:20:50.615123 batch 1 running loss 87.55347031796789 loss 87.55347031796789\n",
      "   2023-10-29 14:26:01.408647 batch 701 running loss 104.34355875751663 loss 103.66528629818761\n",
      "epoch: 70, AP: 0.3181306109572317\n",
      "   2023-10-29 14:26:42.220597 batch 1 running loss 90.66727408972304 loss 90.66727408972304\n",
      "   2023-10-29 14:31:53.945082 batch 701 running loss 99.6395780614312 loss 97.08674754434259\n",
      "epoch: 71, AP: 0.3139976333890132\n",
      "   2023-10-29 14:32:34.622145 batch 1 running loss 98.23491407244722 loss 98.23491407244722\n",
      "   2023-10-29 14:37:45.715294 batch 701 running loss 102.77768031219401 loss 103.85367125358127\n",
      "epoch: 72, AP: 0.31070413661353513\n",
      "   2023-10-29 14:38:22.020888 batch 1 running loss 89.59788705456063 loss 89.59788705456063\n",
      "   2023-10-29 14:43:19.720611 batch 701 running loss 102.54146003330071 loss 102.51269404541058\n",
      "epoch: 73, AP: 0.31697828932703137\n",
      "   2023-10-29 14:43:58.513880 batch 1 running loss 103.07562745510755 loss 103.07562745510755\n",
      "   2023-10-29 14:49:10.418712 batch 701 running loss 97.91488912786507 loss 98.82433964497693\n",
      "epoch: 74, AP: 0.31709723796285383\n",
      "   2023-10-29 14:49:51.518277 batch 1 running loss 103.55507899570881 loss 103.55507899570881\n",
      "   2023-10-29 14:55:02.705019 batch 701 running loss 97.98957719309273 loss 107.01818294166021\n",
      "epoch: 75, AP: 0.3186947205636647\n",
      "   2023-10-29 14:55:43.807938 batch 1 running loss 87.8955485148766 loss 87.8955485148766\n",
      "   2023-10-29 15:00:56.053030 batch 701 running loss 99.5571729759364 loss 111.77093337594332\n",
      "epoch: 76, AP: 0.3224321178992635\n",
      "   2023-10-29 15:01:37.020050 batch 1 running loss 97.49725907367406 loss 97.49725907367406\n",
      "   2023-10-29 15:06:30.313821 batch 701 running loss 93.90080977285486 loss 95.36972981691247\n",
      "epoch: 77, AP: 0.3163304303750603\n",
      "   2023-10-29 15:07:11.915906 batch 1 running loss 102.94269385607512 loss 102.94269385607512\n",
      "   2023-10-29 15:12:22.817134 batch 701 running loss 90.84034674286596 loss 93.08995513958578\n",
      "epoch: 78, AP: 0.319934933574039\n",
      "   2023-10-29 15:13:04.117934 batch 1 running loss 93.26734899278402 loss 93.26734899278402\n",
      "   2023-10-29 15:18:14.824276 batch 701 running loss 89.52403407588093 loss 85.70628304544391\n",
      "epoch: 79, AP: 0.31919007090597074\n",
      "   2023-10-29 15:18:55.418872 batch 1 running loss 87.1780651928351 loss 87.1780651928351\n",
      "   2023-10-29 15:24:08.146769 batch 701 running loss 103.00250217067462 loss 106.2817556868734\n",
      "epoch: 80, AP: 0.31571444464491655\n",
      "   2023-10-29 15:24:48.821317 batch 1 running loss 112.26092533621377 loss 112.26092533621377\n",
      "   2023-10-29 15:30:00.714229 batch 701 running loss 97.97680372100936 loss 97.11230232767251\n",
      "epoch: 81, AP: 0.31172740690014633\n",
      "   2023-10-29 15:30:41.654950 batch 1 running loss 96.7904269863065 loss 96.7904269863065\n",
      "   2023-10-29 15:35:53.014998 batch 701 running loss 93.46705095427725 loss 92.22004031165793\n",
      "epoch: 82, AP: 0.3093716368238034\n",
      "   2023-10-29 15:36:31.916399 batch 1 running loss 88.22980364920915 loss 88.22980364920915\n",
      "   2023-10-29 15:41:44.515227 batch 701 running loss 95.62247619938559 loss 93.94391894588816\n",
      "epoch: 83, AP: 0.31270548499071293\n",
      "   2023-10-29 15:42:24.130825 batch 1 running loss 88.17353104630628 loss 88.17353104630628\n",
      "   2023-10-29 15:47:35.630935 batch 701 running loss 91.71434376880794 loss 88.06330122147048\n",
      "epoch: 84, AP: 0.3069374016655251\n",
      "   2023-10-29 15:48:16.143863 batch 1 running loss 95.04019741604549 loss 95.04019741604549\n",
      "   2023-10-29 15:53:26.996723 batch 701 running loss 89.43599122751326 loss 87.27489515504371\n",
      "epoch: 85, AP: 0.317006747227894\n",
      "   2023-10-29 15:54:07.043177 batch 1 running loss 87.41977878841016 loss 87.41977878841016\n",
      "   2023-10-29 15:59:18.796763 batch 701 running loss 91.0989270112092 loss 98.15375861485983\n",
      "epoch: 86, AP: 0.3163051134259912\n",
      "   2023-10-29 15:59:59.638141 batch 1 running loss 78.52540538634861 loss 78.52540538634861\n",
      "   2023-10-29 16:05:11.315942 batch 701 running loss 84.09445303776096 loss 71.52941362678982\n",
      "epoch: 87, AP: 0.3167972097096242\n",
      "   2023-10-29 16:05:52.014416 batch 1 running loss 100.28335990490602 loss 100.28335990490602\n",
      "   2023-10-29 16:11:04.821844 batch 701 running loss 93.03850362070345 loss 92.82927330324449\n",
      "epoch: 88, AP: 0.3148167952017749\n",
      "   2023-10-29 16:11:45.613498 batch 1 running loss 92.16527464770618 loss 92.16527464770618\n",
      "   2023-10-29 16:17:00.496735 batch 701 running loss 87.39990267427763 loss 91.42377064095822\n",
      "epoch: 89, AP: 0.3170713745631477\n",
      "   2023-10-29 16:17:41.215506 batch 1 running loss 86.31740538231814 loss 86.31740538231814\n",
      "   2023-10-29 16:22:53.244231 batch 701 running loss 97.65550736218228 loss 99.99666062660575\n",
      "epoch: 90, AP: 0.3044401847125921\n",
      "   2023-10-29 16:23:33.814967 batch 1 running loss 95.05713897158091 loss 95.05713897158091\n",
      "   2023-10-29 16:28:44.213942 batch 701 running loss 94.23611620225077 loss 92.00944734850984\n",
      "epoch: 91, AP: 0.3088319822161959\n",
      "   2023-10-29 16:29:25.117749 batch 1 running loss 94.75975176661164 loss 94.75975176661164\n",
      "   2023-10-29 16:34:36.713346 batch 701 running loss 91.98066215501228 loss 99.35461007163758\n",
      "epoch: 92, AP: 0.30464091665055726\n",
      "   2023-10-29 16:35:15.021534 batch 1 running loss 87.98619069306744 loss 87.98619069306744\n",
      "   2023-10-29 16:40:26.556828 batch 701 running loss 94.5248172232889 loss 86.00308505936187\n",
      "epoch: 93, AP: 0.30791030882602155\n",
      "   2023-10-29 16:41:06.849995 batch 1 running loss 93.94630050759272 loss 93.94630050759272\n",
      "   2023-10-29 16:46:19.819282 batch 701 running loss 89.81412835083816 loss 89.88281448319484\n",
      "epoch: 94, AP: 0.3083371073453378\n",
      "   2023-10-29 16:47:00.148824 batch 1 running loss 82.80313816921999 loss 82.80313816921999\n",
      "   2023-10-29 16:52:10.396768 batch 701 running loss 87.92032842771599 loss 85.49552562497261\n",
      "epoch: 95, AP: 0.3063393309308111\n",
      "   2023-10-29 16:52:51.014303 batch 1 running loss 93.52655411009249 loss 93.52655411009249\n",
      "   2023-10-29 16:58:03.037701 batch 701 running loss 85.4695861889892 loss 80.123224732879\n",
      "epoch: 96, AP: 0.31044644251327064\n",
      "   2023-10-29 16:58:43.930166 batch 1 running loss 81.34971209024168 loss 81.34971209024168\n",
      "   2023-10-29 17:03:55.511627 batch 701 running loss 83.36478742553619 loss 80.48517402162493\n",
      "epoch: 97, AP: 0.3098544078147788\n",
      "   2023-10-29 17:04:36.526179 batch 1 running loss 80.87141774116859 loss 80.87141774116859\n",
      "   2023-10-29 17:09:48.110369 batch 701 running loss 83.07736873686368 loss 89.95704928181641\n",
      "epoch: 98, AP: 0.31046989897376287\n",
      "   2023-10-29 17:10:28.718909 batch 1 running loss 80.8547191901244 loss 80.8547191901244\n",
      "   2023-10-29 17:15:40.129251 batch 701 running loss 84.25516310183404 loss 89.36664656056766\n",
      "epoch: 99, AP: 0.31049297931080233\n",
      "   2023-10-29 17:16:21.514398 batch 1 running loss 80.55971720403872 loss 80.55971720403872\n",
      "   2023-10-29 17:21:33.611495 batch 701 running loss 89.18944299963724 loss 89.27830587602082\n",
      "epoch: 100, AP: 0.30463166996342006\n",
      "   2023-10-29 17:22:14.544127 batch 1 running loss 85.80620419841648 loss 85.80620419841648\n",
      "   2023-10-29 17:27:26.251875 batch 701 running loss 91.67822468233065 loss 90.60162468546513\n",
      "epoch: 101, AP: 0.29746037634223854\n",
      "   2023-10-29 17:28:06.519470 batch 1 running loss 91.46823088755261 loss 91.46823088755261\n",
      "   2023-10-29 17:33:19.315021 batch 701 running loss 86.05030196327134 loss 85.82604723108138\n",
      "epoch: 102, AP: 0.30281498131978\n",
      "   2023-10-29 17:33:57.823995 batch 1 running loss 98.1369429207251 loss 98.1369429207251\n",
      "   2023-10-29 17:39:10.545212 batch 701 running loss 85.97010703325022 loss 81.45041726007197\n",
      "epoch: 103, AP: 0.3011196858866725\n",
      "   2023-10-29 17:39:51.014136 batch 1 running loss 85.93234438542419 loss 85.93234438542419\n",
      "   2023-10-29 17:45:01.651472 batch 701 running loss 84.95913984838663 loss 81.53267971497259\n",
      "epoch: 104, AP: 0.30883425960872296\n",
      "   2023-10-29 17:45:42.651509 batch 1 running loss 79.70863677624064 loss 79.70863677624064\n",
      "   2023-10-29 17:50:54.647980 batch 701 running loss 82.05873533816425 loss 92.91121033030532\n",
      "epoch: 105, AP: 0.3068674640659076\n",
      "   2023-10-29 17:51:35.704565 batch 1 running loss 84.92103366411624 loss 84.92103366411624\n",
      "   2023-10-29 17:56:48.712731 batch 701 running loss 75.46654190337057 loss 71.77662000493659\n",
      "epoch: 106, AP: 0.30971063778757424\n",
      "   2023-10-29 17:57:29.996750 batch 1 running loss 74.81022898833021 loss 74.81022898833021\n",
      "   2023-10-29 18:02:41.410947 batch 701 running loss 76.2823112024006 loss 73.21727273179934\n",
      "epoch: 107, AP: 0.3073617804810727\n",
      "   2023-10-29 18:03:21.696874 batch 1 running loss 81.51832908185045 loss 81.51832908185045\n",
      "   2023-10-29 18:08:33.659350 batch 701 running loss 79.30446855940141 loss 79.6260657979863\n",
      "epoch: 108, AP: 0.3059971986799398\n",
      "   2023-10-29 18:09:14.905496 batch 1 running loss 69.33168379054254 loss 69.33168379054254\n",
      "   2023-10-29 18:14:26.251643 batch 701 running loss 78.73301551660273 loss 74.18229953819493\n",
      "epoch: 109, AP: 0.30777475421353667\n",
      "   2023-10-29 18:15:06.718318 batch 1 running loss 74.69897167325232 loss 74.69897167325232\n",
      "   2023-10-29 18:20:19.013620 batch 701 running loss 86.77483321129478 loss 84.53884361066494\n",
      "epoch: 110, AP: 0.2949365202115843\n",
      "   2023-10-29 18:21:00.015650 batch 1 running loss 88.99703667725778 loss 88.99703667725778\n",
      "   2023-10-29 18:26:11.916188 batch 701 running loss 83.54967017287939 loss 84.15558967073174\n",
      "epoch: 111, AP: 0.29884605578687584\n",
      "   2023-10-29 18:26:51.118612 batch 1 running loss 73.89262494230931 loss 73.89262494230931\n",
      "   2023-10-29 18:32:03.220478 batch 701 running loss 82.54894515385001 loss 77.58835440850581\n",
      "epoch: 112, AP: 0.2988740441154525\n",
      "   2023-10-29 18:32:41.811276 batch 1 running loss 77.49798872414874 loss 77.49798872414874\n",
      "   2023-10-29 18:37:54.445698 batch 701 running loss 83.07100144848467 loss 87.76722289897744\n",
      "epoch: 113, AP: 0.29790004028992034\n",
      "   2023-10-29 18:38:35.496725 batch 1 running loss 73.87704333105637 loss 73.87704333105637\n",
      "   2023-10-29 18:43:48.523430 batch 701 running loss 77.5313217860151 loss 79.38332542534486\n",
      "epoch: 114, AP: 0.2987031546214689\n",
      "   2023-10-29 18:44:29.335539 batch 1 running loss 69.93382260952973 loss 69.93382260952973\n",
      "   2023-10-29 18:49:42.227255 batch 701 running loss 76.23048344404032 loss 79.93237609189993\n",
      "epoch: 115, AP: 0.3017285334648061\n",
      "   2023-10-29 18:50:22.330820 batch 1 running loss 64.7914484348475 loss 64.7914484348475\n",
      "   2023-10-29 18:55:35.016250 batch 701 running loss 71.72831355079329 loss 73.85802076862043\n",
      "epoch: 116, AP: 0.30297772555397984\n",
      "   2023-10-29 18:56:14.919635 batch 1 running loss 68.77783378213155 loss 68.77783378213155\n",
      "   2023-10-29 19:01:27.214860 batch 701 running loss 73.78625752596028 loss 73.67106484095675\n",
      "epoch: 117, AP: 0.30299343333795126\n",
      "   2023-10-29 19:02:07.235507 batch 1 running loss 68.23124489119488 loss 68.23124489119488\n",
      "   2023-10-29 19:07:22.821793 batch 701 running loss 72.31807247314141 loss 68.97819752677147\n",
      "epoch: 118, AP: 0.30337749527960794\n",
      "   2023-10-29 19:08:03.296734 batch 1 running loss 65.32466183702385 loss 65.32466183702385\n",
      "   2023-10-29 19:13:15.111638 batch 701 running loss 70.02820031615792 loss 69.56332752824446\n",
      "epoch: 119, AP: 0.3038755123155359\n",
      "   2023-10-29 19:13:55.124780 batch 1 running loss 63.543229482515784 loss 63.543229482515784\n",
      "   2023-10-29 19:19:08.696759 batch 701 running loss 82.43637251323383 loss 77.1792457925177\n",
      "epoch: 120, AP: 0.296482831681274\n",
      "   2023-10-29 19:19:49.920849 batch 1 running loss 77.24079174691917 loss 77.24079174691917\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "from timm.loss import AsymmetricLossMultiLabel\n",
    "from transformers import set_seed\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "\n",
    "model = Network()\n",
    "criterion = AsymmetricLossMultiLabel()\n",
    "\n",
    "epochs = 150\n",
    "model = model.to(CUDA_DEV)\n",
    "criterion = criterion.to(CUDA_DEV)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, eta_min=1e-7)\n",
    "\n",
    "best = 0\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_epoch(model, train_dataloader, criterion, optimizer, scheduler, print_loss=True, iteration_step=700, epoch=epoch)\n",
    "    track_idxs, predictions, targets = predict_train(model, val_dataloader)\n",
    "    ap = sklearn.metrics.average_precision_score(targets, predictions)\n",
    "    print(f\"epoch: {epoch}, AP: {ap}\")\n",
    "    if (ap > best):\n",
    "        best = ap\n",
    "        best_model = model\n",
    "        if (epoch > 20):\n",
    "            track_idxs, predictions = predict(model, test_dataloader)\n",
    "            for i, c in enumerate(predictions.argmax(-1)):\n",
    "                probs = np.array([1 + dict_tags[c].get(t, 0) for t in np.arange(predictions.shape[1])])\n",
    "                probs[c] = 2\n",
    "                predictions[i] = predictions[i] * probs\n",
    "                predictions[i] /= predictions[i].sum()\n",
    "                        \n",
    "            predictions_df = pd.DataFrame([\n",
    "                {'track': track, 'prediction': ','.join([str(p) for p in probs])}\n",
    "                for track, probs in zip(track_idxs, predictions)\n",
    "            ])\n",
    "            predictions_df.to_csv(f'subs/prediction_asym.csv', index=False)\n",
    "            torch.save(best_model.state_dict(), f'models/models_asym.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa898f51-c075-47fc-95a0-b4f95ed655b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP: 0.3199148484354201\n",
      "POST AP: 0.32061250757510396\n",
      "POST-1 AP: 0.3215476797234166\n"
     ]
    }
   ],
   "source": [
    "track_idxs, predictions, targets = predict_train(best_model, val_dataloader)\n",
    "ap = sklearn.metrics.average_precision_score(targets, predictions)\n",
    "print(f\"AP: {ap}\")\n",
    "\n",
    "for i, c in enumerate(predictions.argmax(-1)):\n",
    "    probs = np.array([1 + dict_tags[c].get(t, 0) for t in np.arange(predictions.shape[1])])\n",
    "    probs[c] = 2\n",
    "    predictions[i] = predictions[i] * probs\n",
    "    predictions[i] /= predictions[i].sum()\n",
    "ap = sklearn.metrics.average_precision_score(targets, predictions)\n",
    "\n",
    "print(f\"POST AP: {ap}\")\n",
    "\n",
    "for i, c in enumerate(predictions.argmax(-1)):\n",
    "    probs = np.array([1 + dict_tags[c].get(t, 0) for t in np.arange(predictions.shape[1])])\n",
    "    probs[c] = 2\n",
    "    predictions[i] = predictions[i] * probs\n",
    "    predictions[i] /= predictions[i].sum()\n",
    "ap = sklearn.metrics.average_precision_score(targets, predictions)\n",
    "\n",
    "print(f\"POST-1 AP: {ap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4daf63c5-45b4-4cb8-99f1-534fc8203caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_idxs, predictions = predict(best_model, test_dataloader)\n",
    "\n",
    "for i, c in enumerate(predictions.argmax(-1)):\n",
    "    probs = np.array([1 + dict_tags[c].get(t, 0) for t in np.arange(predictions.shape[1])])\n",
    "    probs[c] = 2\n",
    "    predictions[i] = predictions[i] * probs\n",
    "    predictions[i] /= predictions[i].sum()\n",
    "    \n",
    "for i, c in enumerate(predictions.argmax(-1)):\n",
    "    probs = np.array([1 + dict_tags[c].get(t, 0) for t in np.arange(predictions.shape[1])])\n",
    "    probs[c] = 2\n",
    "    predictions[i] = predictions[i] * probs\n",
    "    predictions[i] /= predictions[i].sum()\n",
    "\n",
    "predictions_df = pd.DataFrame([\n",
    "    {'track': track, 'prediction': ','.join([str(p) for p in probs])}\n",
    "    for track, probs in zip(track_idxs, predictions)\n",
    "])\n",
    "predictions_df.to_csv(f'prediction_best_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ae520e-08d1-4a77-85e3-d0023c8fc8a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ya",
   "language": "python",
   "name": "ya"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
