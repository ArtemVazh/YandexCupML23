{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "340aceb6-4360-49ac-8381-bb42055d6146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.remove('/home/jovyan/.imgenv-lm-poly-0/lib/python3.7/site-packages')\n",
    "os.environ['PYTHONPATH'] = '/home/user/conda/envs/ya/lib/python3.10/site-packages'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32801790-6619-4141-b909-a70de5e00071",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_DEV = 0\n",
    "NUM_TAGS = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29f35090-eb3d-464b-b46f-de30c4d42d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "144deed2-ca15-41aa-9225-6edf01d1003e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "tags = [[int(i) for i in x.split(',')] for x in df_train.tags.values]\n",
    "dict_tags = {}\n",
    "for cls_tags in tags:\n",
    "    for c in cls_tags:\n",
    "        if c not in dict_tags.keys():\n",
    "            dict_tags[c] = Counter(cls_tags)\n",
    "        else:\n",
    "            dict_tags[c].update(Counter(cls_tags))\n",
    "            \n",
    "for tag in dict_tags.keys():\n",
    "    del dict_tags[tag][tag]\n",
    "    n = np.sum(list(dict_tags[tag].values()))\n",
    "    for t in dict_tags[tag].keys():\n",
    "        dict_tags[tag][t] = dict_tags[tag][t]/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b46b7d4b-785e-4187-b2bc-6c620c7b8cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 76715/76715 [03:40<00:00, 347.73it/s]\n"
     ]
    }
   ],
   "source": [
    "track_idx2embeds = {}\n",
    "for fn in tqdm(glob('track_embeddings/*')):\n",
    "    name = fn.split('/')[1].split('.')[0]\n",
    "    if name == \"track_embeddings\":\n",
    "        continue\n",
    "    track_idx = int(name)\n",
    "    embeds = np.load(fn)\n",
    "    track_idx2embeds[track_idx] = embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d1b5cae-60ae-4584-a6bb-4f6b833929aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaggingDataset(Dataset):\n",
    "    def __init__(self, df, aug=0, testing=False):\n",
    "        self.df = df\n",
    "        self.testing = testing\n",
    "        self.aug = aug\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        track_idx = row.track\n",
    "        embeds = track_idx2embeds[track_idx]\n",
    "        if self.testing:\n",
    "            return track_idx, embeds\n",
    "        tags = [int(x) for x in row.tags.split(',')]\n",
    "        target = np.zeros(NUM_TAGS)\n",
    "        target[tags] = 1\n",
    "        \n",
    "        if np.random.choice([0, 1], p=[1 - self.aug, self.aug]):\n",
    "            s = np.random.uniform(0.0, 0.4)\n",
    "            e = np.random.uniform(s+0.1, 1)\n",
    "            s = int(s * embeds.shape[0])\n",
    "            e = int(e * embeds.shape[0])\n",
    "            embeds = embeds[s:e]\n",
    "        \n",
    "        return track_idx, embeds, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca9ac5cf-a481-4918-bbeb-ecf077c681ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TaggingDataset(df_train, aug=0.6)\n",
    "val_dataset = TaggingDataset(df_train[-1000:])\n",
    "\n",
    "test_dataset = TaggingDataset(df_test, testing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31c659b7-ee4b-44da-a715-b7abced07279",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, emb_dim=768, mult=4, p=0.0):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(emb_dim, emb_dim * mult),\n",
    "            nn.Dropout(p),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(emb_dim * mult, emb_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "    \n",
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Sequential(\n",
    "            nn.Linear(embedding_size, embedding_size),\n",
    "            nn.LayerNorm(embedding_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embedding_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_logits = self.attn(x)\n",
    "        if mask is not None:\n",
    "            attn_logits[mask] = -float('inf')\n",
    "        attn_weights = torch.softmax(attn_logits, dim=1)\n",
    "        x = x * attn_weights\n",
    "        x = x.sum(dim=1)\n",
    "        return x\n",
    "    \n",
    "class Network(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes = NUM_TAGS,\n",
    "        input_dim = 768,\n",
    "        hidden_dim = 512\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.proj = FeedForward(input_dim)\n",
    "        self.bn = nn.BatchNorm1d(input_dim)\n",
    "        self.ln = nn.LayerNorm(input_dim)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=768, nhead=12, activation=\"gelu\", batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=6)\n",
    "        self.poooling = AttentionPooling(input_dim)\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "               \n",
    "    def forward(self, embeds):\n",
    "        embeds = self.proj(embeds)\n",
    "        src_key_padding_mask = (embeds.mean(-1) == -1)\n",
    "        embeds = self.ln(embeds)\n",
    "        x = self.transformer_encoder(embeds, src_key_padding_mask=src_key_padding_mask)\n",
    "        x = self.bn(self.poooling(x, mask=src_key_padding_mask))\n",
    "        outs = self.fc(x)\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c70bf034-7966-4f44-9f2e-dcaf0f8a8184",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, scheduler, print_loss=True, iteration_step=100, epoch=0):\n",
    "    model.train()\n",
    "    running_loss = None\n",
    "    alpha = 0.8\n",
    "    iters = len(loader)\n",
    "    for iteration,data in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        track_idxs, embeds, target = data\n",
    "        embeds = [x.to(CUDA_DEV) for x in embeds]\n",
    "        embeds = pad_sequence(embeds, padding_value=-1, batch_first=True)[:, :64, :]\n",
    "        target = target.to(CUDA_DEV)\n",
    "        pred_logits = model(embeds)\n",
    "        pred_probs = torch.sigmoid(pred_logits)\n",
    "        ce_loss = criterion(pred_logits, target)\n",
    "            \n",
    "        ce_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        scheduler.step(epoch + iteration / iters)\n",
    "        \n",
    "        if running_loss is None:\n",
    "            running_loss = ce_loss.item()\n",
    "        else:\n",
    "            running_loss = alpha * running_loss + (1 - alpha) * ce_loss.item()\n",
    "        if (iteration % iteration_step == 0) and print_loss:\n",
    "            print('   {} batch {} running loss {} loss {}'.format(\n",
    "                datetime.now(), iteration + 1, running_loss, ce_loss.item()\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e624b0b-8daf-4702-a5de-c667fcd53121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, loader):\n",
    "    model.eval()\n",
    "    track_idxs = []\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            track_idx, embeds = data\n",
    "            embeds = [x.to(CUDA_DEV) for x in embeds]\n",
    "            embeds = pad_sequence(embeds, padding_value=-1, batch_first=True)[:, :64, :]\n",
    "            pred_logits = model(embeds)\n",
    "            pred_probs = torch.sigmoid(pred_logits)\n",
    "            predictions.append(pred_probs.cpu().numpy())\n",
    "            track_idxs.append(track_idx.numpy())\n",
    "    predictions = np.vstack(predictions)\n",
    "    track_idxs = np.vstack(track_idxs).ravel()\n",
    "    return track_idxs, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37c01c5a-6d40-47d3-a535-d3349b41139e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def predict_train(model, loader):\n",
    "    model.eval()\n",
    "    track_idxs = []\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            track_idx, embeds, target = data\n",
    "            embeds = [x.to(CUDA_DEV) for x in embeds]\n",
    "            embeds = pad_sequence(embeds, padding_value=-1, batch_first=True)[:, :64, :]\n",
    "            pred_logits = model(embeds)\n",
    "            pred_probs = torch.sigmoid(pred_logits)\n",
    "            predictions.append(pred_probs.cpu().numpy())\n",
    "            track_idxs.append(track_idx.numpy())\n",
    "            targets.append(target.numpy())\n",
    "    predictions = np.vstack(predictions)\n",
    "    targets = np.vstack(targets)\n",
    "    track_idxs = np.vstack(track_idxs).ravel()\n",
    "    return track_idxs, predictions, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4decde5-005a-4820-804c-cf4d110c799e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(b):\n",
    "    track_idxs = torch.from_numpy(np.vstack([x[0] for x in b]))\n",
    "    embeds = [torch.from_numpy(x[1]) for x in b]\n",
    "    targets = np.vstack([x[2] for x in b])\n",
    "    targets = torch.from_numpy(targets)\n",
    "    return track_idxs, embeds, targets\n",
    "\n",
    "def collate_fn_test(b):\n",
    "    track_idxs = torch.from_numpy(np.vstack([x[0] for x in b]))\n",
    "    embeds = [torch.from_numpy(x[1]) for x in b]\n",
    "    return track_idxs, embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62124a85-3bcc-4eb8-a0d7-931c8685d1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=128, shuffle=False, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False, collate_fn=collate_fn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "448ff0ee-2a61-4761-8d54-8b555bca7a09",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2023-10-28 15:51:54.157293 batch 1 running loss 0.7322065504702948 loss 0.7322065504702948\n",
      "   2023-10-28 15:55:36.010882 batch 701 running loss 0.6814307027103386 loss 0.6808858894363432\n",
      "epoch: 0, AP: 0.10864242853223499\n",
      "   2023-10-28 15:56:09.331872 batch 1 running loss 0.6787031817084128 loss 0.6787031817084128\n",
      "   2023-10-28 16:00:05.211074 batch 701 running loss 0.6436508518543024 loss 0.6436927197169445\n",
      "epoch: 1, AP: 0.14382231291364736\n",
      "   2023-10-28 16:00:51.419497 batch 1 running loss 0.6352676800907278 loss 0.6352676800907278\n",
      "   2023-10-28 16:05:39.336838 batch 701 running loss 0.5683478154591668 loss 0.5682148094990147\n",
      "epoch: 2, AP: 0.17388842185325423\n",
      "   2023-10-28 16:06:25.996801 batch 1 running loss 0.5578140101828106 loss 0.5578140101828106\n",
      "   2023-10-28 16:11:05.540540 batch 701 running loss 0.48398303282026045 loss 0.48373106292955326\n",
      "epoch: 3, AP: 0.185641242943417\n",
      "   2023-10-28 16:11:43.320579 batch 1 running loss 0.47330740658344617 loss 0.47330740658344617\n",
      "   2023-10-28 16:16:47.313893 batch 701 running loss 0.41398572239682757 loss 0.4148506314752469\n",
      "epoch: 4, AP: 0.19736847165555968\n",
      "   2023-10-28 16:17:32.196769 batch 1 running loss 0.4090572208698404 loss 0.4090572208698404\n",
      "   2023-10-28 16:22:04.720063 batch 701 running loss 0.36429059137996184 loss 0.3630899548019277\n",
      "epoch: 5, AP: 0.21826076354214147\n",
      "   2023-10-28 16:22:50.820586 batch 1 running loss 0.3590899288907843 loss 0.3590899288907843\n",
      "   2023-10-28 16:27:37.543390 batch 701 running loss 0.33262425439632143 loss 0.3328972258282641\n",
      "epoch: 6, AP: 0.21950713475507105\n",
      "   2023-10-28 16:28:20.701136 batch 1 running loss 0.3286418160504315 loss 0.3286418160504315\n",
      "   2023-10-28 16:33:00.596767 batch 701 running loss 0.3158805338503831 loss 0.3150926090857292\n",
      "epoch: 7, AP: 0.22680643129184705\n",
      "   2023-10-28 16:33:43.833552 batch 1 running loss 0.31409030520082803 loss 0.31409030520082803\n",
      "   2023-10-28 16:38:49.018498 batch 701 running loss 0.30942163764100894 loss 0.30907996543272\n",
      "epoch: 8, AP: 0.22905533401010708\n",
      "   2023-10-28 16:39:31.017320 batch 1 running loss 0.30907739395538825 loss 0.30907739395538825\n",
      "   2023-10-28 16:44:22.214261 batch 701 running loss 0.30666661556239766 loss 0.30592571376549843\n",
      "epoch: 9, AP: 0.22865472922491922\n",
      "   2023-10-28 16:45:08.519433 batch 1 running loss 0.30574117256214806 loss 0.30574117256214806\n",
      "   2023-10-28 16:50:12.424793 batch 701 running loss 0.23266741853120945 loss 0.23097647385167364\n",
      "epoch: 10, AP: 0.23040069308563296\n",
      "   2023-10-28 16:50:58.614882 batch 1 running loss 0.2224917039990108 loss 0.2224917039990108\n",
      "   2023-10-28 16:56:01.518441 batch 701 running loss 0.17363192977607836 loss 0.17257801581157436\n",
      "epoch: 11, AP: 0.2312622817562195\n",
      "   2023-10-28 16:56:47.820647 batch 1 running loss 0.16773797888967623 loss 0.16773797888967623\n",
      "   2023-10-28 17:01:51.417491 batch 701 running loss 0.13582006164640728 loss 0.13680939068052034\n",
      "epoch: 12, AP: 0.2423882298935447\n",
      "   2023-10-28 17:02:37.320476 batch 1 running loss 0.12922444031854896 loss 0.12922444031854896\n",
      "   2023-10-28 17:07:25.217570 batch 701 running loss 0.11185810842280172 loss 0.1113463673264058\n",
      "epoch: 13, AP: 0.2443408952491772\n",
      "   2023-10-28 17:08:11.920318 batch 1 running loss 0.10852754610619703 loss 0.10852754610619703\n",
      "   2023-10-28 17:13:16.215261 batch 701 running loss 0.09753758801376819 loss 0.09925227073609078\n",
      "epoch: 14, AP: 0.26633059074895626\n",
      "   2023-10-28 17:14:02.396774 batch 1 running loss 0.0957637721493215 loss 0.0957637721493215\n",
      "   2023-10-28 17:19:06.039816 batch 701 running loss 0.0880806535918798 loss 0.08999322207608818\n",
      "epoch: 15, AP: 0.27251091372353764\n",
      "   2023-10-28 17:19:52.716905 batch 1 running loss 0.08878830260972725 loss 0.08878830260972725\n",
      "   2023-10-28 17:24:40.616313 batch 701 running loss 0.08175968137114924 loss 0.08050354276155425\n",
      "epoch: 16, AP: 0.28147251861987777\n",
      "   2023-10-28 17:25:27.614137 batch 1 running loss 0.07913957558700521 loss 0.07913957558700521\n",
      "   2023-10-28 17:30:27.712010 batch 701 running loss 0.07914507765389797 loss 0.07961121673855254\n",
      "epoch: 17, AP: 0.2881877750833271\n",
      "   2023-10-28 17:31:06.515303 batch 1 running loss 0.08366208990348145 loss 0.08366208990348145\n",
      "   2023-10-28 17:36:05.019558 batch 701 running loss 0.07783175027898702 loss 0.08357529464086277\n",
      "epoch: 18, AP: 0.2888089116454746\n",
      "   2023-10-28 17:36:50.950544 batch 1 running loss 0.07825016685739683 loss 0.07825016685739683\n",
      "   2023-10-28 17:41:53.324089 batch 701 running loss 0.07638601044804949 loss 0.07722590065023338\n",
      "epoch: 19, AP: 0.28781731537044647\n",
      "   2023-10-28 17:42:39.829227 batch 1 running loss 0.08000249361246825 loss 0.08000249361246825\n",
      "   2023-10-28 17:47:43.020388 batch 701 running loss 0.06712413755032387 loss 0.06693497448784896\n",
      "epoch: 20, AP: 0.27695100623656344\n",
      "   2023-10-28 17:48:28.851950 batch 1 running loss 0.07230386287675672 loss 0.07230386287675672\n",
      "   2023-10-28 17:53:26.496746 batch 701 running loss 0.06000011422878801 loss 0.06039375862196872\n",
      "epoch: 21, AP: 0.2940325452589907\n",
      "   2023-10-28 17:54:02.544611 batch 1 running loss 0.058495155469385884 loss 0.058495155469385884\n",
      "   2023-10-28 17:59:07.422974 batch 701 running loss 0.05580399717378031 loss 0.05455056270174491\n",
      "epoch: 22, AP: 0.2970139036874312\n",
      "   2023-10-28 17:59:49.596779 batch 1 running loss 0.05615661942179084 loss 0.05615661942179084\n",
      "   2023-10-28 18:04:39.314142 batch 701 running loss 0.05238757354675028 loss 0.04992962686865261\n",
      "epoch: 23, AP: 0.31253154351582757\n",
      "   2023-10-28 18:05:25.311565 batch 1 running loss 0.05206688036038949 loss 0.05206688036038949\n",
      "   2023-10-28 18:10:13.115983 batch 701 running loss 0.05302143095516932 loss 0.057659716113167114\n",
      "epoch: 24, AP: 0.32257109527396943\n",
      "   2023-10-28 18:10:59.415930 batch 1 running loss 0.048659715247430596 loss 0.048659715247430596\n",
      "   2023-10-28 18:15:46.117223 batch 701 running loss 0.05037962736417632 loss 0.05320741692773612\n",
      "epoch: 25, AP: 0.3295500924381304\n",
      "   2023-10-28 18:16:28.532888 batch 1 running loss 0.04440691023309773 loss 0.04440691023309773\n",
      "   2023-10-28 18:21:09.635487 batch 701 running loss 0.04898143842815356 loss 0.04962659059643782\n",
      "epoch: 26, AP: 0.3313830718603856\n",
      "   2023-10-28 18:21:48.917688 batch 1 running loss 0.053554395757728024 loss 0.053554395757728024\n",
      "   2023-10-28 18:26:37.110342 batch 701 running loss 0.045907634072884076 loss 0.04701548016472272\n",
      "epoch: 27, AP: 0.3356725083936771\n",
      "   2023-10-28 18:27:23.631604 batch 1 running loss 0.046449160179927805 loss 0.046449160179927805\n",
      "   2023-10-28 18:32:07.524874 batch 701 running loss 0.0464914134499496 loss 0.04811597369903261\n",
      "epoch: 28, AP: 0.3391846042116097\n",
      "   2023-10-28 18:32:50.218743 batch 1 running loss 0.047804654502796584 loss 0.047804654502796584\n",
      "   2023-10-28 18:37:41.915204 batch 701 running loss 0.047676668268363316 loss 0.04566301927676014\n",
      "epoch: 29, AP: 0.3378677383750624\n",
      "   2023-10-28 18:38:27.322651 batch 1 running loss 0.04670190465367402 loss 0.04670190465367402\n",
      "   2023-10-28 18:43:13.611333 batch 701 running loss 0.047451772967052705 loss 0.05221706050086361\n",
      "epoch: 30, AP: 0.3203785312238034\n",
      "   2023-10-28 18:44:00.518122 batch 1 running loss 0.04518556347958391 loss 0.04518556347958391\n",
      "   2023-10-28 18:48:46.637210 batch 701 running loss 0.04509350744990309 loss 0.04356957907694639\n",
      "epoch: 31, AP: 0.3341169527004888\n",
      "   2023-10-28 18:49:32.724657 batch 1 running loss 0.042725868483715956 loss 0.042725868483715956\n",
      "   2023-10-28 18:54:35.296739 batch 701 running loss 0.04497783336549195 loss 0.046059695551670055\n",
      "epoch: 32, AP: 0.34113166695210645\n",
      "   2023-10-28 18:55:21.716701 batch 1 running loss 0.04671063401563025 loss 0.04671063401563025\n",
      "   2023-10-28 19:00:26.296717 batch 701 running loss 0.04433291076295394 loss 0.04865903485348522\n",
      "epoch: 33, AP: 0.3474642633365518\n",
      "   2023-10-28 19:01:12.915134 batch 1 running loss 0.04401873992172334 loss 0.04401873992172334\n",
      "   2023-10-28 19:06:16.621547 batch 701 running loss 0.04394020590017496 loss 0.044720863339673844\n",
      "epoch: 34, AP: 0.3544580627890954\n",
      "   2023-10-28 19:07:02.802980 batch 1 running loss 0.047676459761182954 loss 0.047676459761182954\n",
      "   2023-10-28 19:12:05.353927 batch 701 running loss 0.043300741481561614 loss 0.04723598358740411\n",
      "epoch: 35, AP: 0.359907247693885\n",
      "   2023-10-28 19:12:52.196611 batch 1 running loss 0.045936062247043274 loss 0.045936062247043274\n",
      "   2023-10-28 19:17:55.613979 batch 701 running loss 0.04133253419045477 loss 0.04298015306617309\n",
      "epoch: 36, AP: 0.36905963110791834\n",
      "   2023-10-28 19:18:42.615519 batch 1 running loss 0.04788588987115716 loss 0.04788588987115716\n",
      "   2023-10-28 19:23:28.409877 batch 701 running loss 0.04193500220506067 loss 0.03759948639219379\n",
      "epoch: 37, AP: 0.36842664828243865\n",
      "   2023-10-28 19:24:15.617284 batch 1 running loss 0.04684140094674616 loss 0.04684140094674616\n",
      "   2023-10-28 19:28:55.111228 batch 701 running loss 0.042707946111677844 loss 0.04952645543003964\n",
      "epoch: 38, AP: 0.3698677438345599\n",
      "   2023-10-28 19:29:32.527911 batch 1 running loss 0.0416278310443321 loss 0.0416278310443321\n",
      "   2023-10-28 19:34:21.515898 batch 701 running loss 0.04046715506189232 loss 0.037633816145419274\n",
      "epoch: 39, AP: 0.3714833239585627\n",
      "   2023-10-28 19:35:07.635139 batch 1 running loss 0.04417875879263633 loss 0.04417875879263633\n",
      "   2023-10-28 19:40:12.918873 batch 701 running loss 0.046304793105901354 loss 0.048863210281266944\n",
      "epoch: 40, AP: 0.3644591142945082\n",
      "   2023-10-28 19:40:59.444957 batch 1 running loss 0.03832859341085326 loss 0.03832859341085326\n",
      "   2023-10-28 19:46:04.596776 batch 701 running loss 0.04472021761875783 loss 0.04285115934223249\n",
      "epoch: 41, AP: 0.37511043894315765\n",
      "   2023-10-28 19:46:50.316624 batch 1 running loss 0.03887349000960594 loss 0.03887349000960594\n",
      "   2023-10-28 19:51:55.936879 batch 701 running loss 0.04295192014008683 loss 0.046837058825922595\n",
      "epoch: 42, AP: 0.3758176865589533\n",
      "   2023-10-28 19:52:41.922887 batch 1 running loss 0.04437089389625526 loss 0.04437089389625526\n",
      "   2023-10-28 19:57:46.801721 batch 701 running loss 0.04183608178675229 loss 0.042356267108694445\n",
      "epoch: 43, AP: 0.3826994216069334\n",
      "   2023-10-28 19:58:32.518334 batch 1 running loss 0.04431018055226699 loss 0.04431018055226699\n",
      "   2023-10-28 20:03:35.830558 batch 701 running loss 0.04223693109754572 loss 0.04028995955140946\n",
      "epoch: 44, AP: 0.3963610391159474\n",
      "   2023-10-28 20:04:21.512920 batch 1 running loss 0.03808439829298904 loss 0.03808439829298904\n",
      "   2023-10-28 20:09:25.345891 batch 701 running loss 0.042019673602161654 loss 0.047069885902066844\n",
      "epoch: 45, AP: 0.3904498154521984\n",
      "   2023-10-28 20:10:11.931105 batch 1 running loss 0.04012759587173331 loss 0.04012759587173331\n",
      "   2023-10-28 20:15:17.642269 batch 701 running loss 0.03958026711961548 loss 0.03844950581979867\n",
      "epoch: 46, AP: 0.40062633272451675\n",
      "   2023-10-28 20:16:03.418929 batch 1 running loss 0.04453001200621376 loss 0.04453001200621376\n",
      "   2023-10-28 20:21:06.822632 batch 701 running loss 0.0433190171456727 loss 0.05018983300770041\n",
      "epoch: 47, AP: 0.40291122055328904\n",
      "   2023-10-28 20:21:52.496712 batch 1 running loss 0.04336376111202778 loss 0.04336376111202778\n",
      "   2023-10-28 20:26:58.420988 batch 701 running loss 0.040838071994021265 loss 0.042482705132313114\n",
      "epoch: 48, AP: 0.4036248138140837\n",
      "   2023-10-28 20:27:43.726935 batch 1 running loss 0.04132943544501244 loss 0.04132943544501244\n",
      "   2023-10-28 20:32:49.030733 batch 701 running loss 0.03859822087213735 loss 0.03655751112293415\n",
      "epoch: 49, AP: 0.40636795479259624\n",
      "   2023-10-28 20:33:35.414481 batch 1 running loss 0.038102865392704466 loss 0.038102865392704466\n",
      "   2023-10-28 20:38:41.015439 batch 701 running loss 0.040883771906060205 loss 0.044659820033056796\n",
      "epoch: 50, AP: 0.3845398175260271\n",
      "   2023-10-28 20:39:26.816234 batch 1 running loss 0.039380910232345356 loss 0.039380910232345356\n",
      "   2023-10-28 20:44:31.747917 batch 701 running loss 0.040407006482119895 loss 0.03959133885289534\n",
      "epoch: 51, AP: 0.38660207815763203\n",
      "   2023-10-28 20:45:18.017105 batch 1 running loss 0.03877446077372798 loss 0.03877446077372798\n",
      "   2023-10-28 20:50:21.396716 batch 701 running loss 0.04013798370102273 loss 0.03773204082926018\n",
      "epoch: 52, AP: 0.4037165536352777\n",
      "   2023-10-28 20:51:07.316575 batch 1 running loss 0.03828795754110609 loss 0.03828795754110609\n",
      "   2023-10-28 20:56:11.316676 batch 701 running loss 0.038635736147860894 loss 0.03849031000374609\n",
      "epoch: 53, AP: 0.41709221978440714\n",
      "   2023-10-28 20:56:57.011262 batch 1 running loss 0.037703478664824486 loss 0.037703478664824486\n",
      "   2023-10-28 21:02:00.904828 batch 701 running loss 0.03835818785152388 loss 0.03858250319108619\n",
      "epoch: 54, AP: 0.4258744559432888\n",
      "   2023-10-28 21:02:46.911309 batch 1 running loss 0.03399665377187966 loss 0.03399665377187966\n",
      "   2023-10-28 21:07:49.618730 batch 701 running loss 0.04000444925853609 loss 0.039599805142519595\n",
      "epoch: 55, AP: 0.43141302166225226\n",
      "   2023-10-28 21:08:36.196688 batch 1 running loss 0.03768410173744419 loss 0.03768410173744419\n",
      "   2023-10-28 21:13:40.609432 batch 701 running loss 0.039013993097919246 loss 0.04469078329962434\n",
      "epoch: 56, AP: 0.4373852680312446\n",
      "   2023-10-28 21:14:27.516733 batch 1 running loss 0.03406404288361764 loss 0.03406404288361764\n",
      "   2023-10-28 21:19:31.716147 batch 701 running loss 0.03893188289315269 loss 0.03856781519788349\n",
      "epoch: 57, AP: 0.4394386095051678\n",
      "   2023-10-28 21:20:17.821323 batch 1 running loss 0.03658784103765478 loss 0.03658784103765478\n",
      "   2023-10-28 21:25:22.209075 batch 701 running loss 0.03920385068661516 loss 0.03986079896849781\n",
      "epoch: 58, AP: 0.44313399354262856\n",
      "   2023-10-28 21:26:08.740889 batch 1 running loss 0.03412815459572288 loss 0.03412815459572288\n",
      "   2023-10-28 21:31:11.648186 batch 701 running loss 0.03863202757756333 loss 0.03803774865451298\n",
      "epoch: 59, AP: 0.4413339573323621\n",
      "   2023-10-28 21:31:58.129729 batch 1 running loss 0.0398979902933192 loss 0.0398979902933192\n",
      "   2023-10-28 21:37:00.829291 batch 701 running loss 0.03848540523172868 loss 0.04191781838120018\n",
      "epoch: 60, AP: 0.4185647910548621\n",
      "   2023-10-28 21:37:46.996669 batch 1 running loss 0.03429482666962752 loss 0.03429482666962752\n",
      "   2023-10-28 21:42:50.296967 batch 701 running loss 0.03836326406924845 loss 0.04231255172712381\n",
      "epoch: 61, AP: 0.4262875852773152\n",
      "   2023-10-28 21:43:36.421824 batch 1 running loss 0.03835878636995227 loss 0.03835878636995227\n",
      "   2023-10-28 21:48:39.514548 batch 701 running loss 0.038226815707089065 loss 0.040782900389940646\n",
      "epoch: 62, AP: 0.44367212733694694\n",
      "   2023-10-28 21:49:25.614266 batch 1 running loss 0.044717136763783305 loss 0.044717136763783305\n",
      "   2023-10-28 21:54:28.850731 batch 701 running loss 0.03863763812322462 loss 0.039149412844401626\n",
      "epoch: 63, AP: 0.4520433095696855\n",
      "   2023-10-28 21:55:15.222337 batch 1 running loss 0.039668381172288464 loss 0.039668381172288464\n",
      "   2023-10-28 22:00:18.417608 batch 701 running loss 0.0372892586852268 loss 0.035009291143617366\n",
      "epoch: 64, AP: 0.4595642092140964\n",
      "   2023-10-28 22:01:04.520433 batch 1 running loss 0.03669082256199244 loss 0.03669082256199244\n",
      "   2023-10-28 22:06:09.818178 batch 701 running loss 0.036793551799636275 loss 0.03365240920412754\n",
      "epoch: 65, AP: 0.4674097778321642\n",
      "   2023-10-28 22:06:56.296740 batch 1 running loss 0.03436096131830216 loss 0.03436096131830216\n",
      "   2023-10-28 22:12:00.796721 batch 701 running loss 0.03774580706060188 loss 0.04143145271534571\n",
      "epoch: 66, AP: 0.4692397210465194\n",
      "   2023-10-28 22:12:46.740383 batch 1 running loss 0.04064149971062675 loss 0.04064149971062675\n",
      "   2023-10-28 22:17:51.311777 batch 701 running loss 0.03638446148296248 loss 0.03864908966678893\n",
      "epoch: 67, AP: 0.47894141651533084\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "from transformers import set_seed\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "\n",
    "model = Network()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "epochs = 68\n",
    "model = model.to(CUDA_DEV)\n",
    "criterion = criterion.to(CUDA_DEV)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, eta_min=1e-7)\n",
    "\n",
    "best = 0\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_epoch(model, train_dataloader, criterion, optimizer, scheduler, print_loss=True, iteration_step=700, epoch=epoch)\n",
    "    track_idxs, predictions, targets = predict_train(model, val_dataloader)\n",
    "    ap = sklearn.metrics.average_precision_score(targets, predictions)\n",
    "    print(f\"epoch: {epoch}, AP: {ap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fa898f51-c075-47fc-95a0-b4f95ed655b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP: 0.47894141651533084\n",
      "POST AP: 0.4677503726306893\n",
      "POST-1 AP: 0.4698290639433038\n"
     ]
    }
   ],
   "source": [
    "track_idxs, predictions, targets = predict_train(model, val_dataloader)\n",
    "ap = sklearn.metrics.average_precision_score(targets, predictions)\n",
    "print(f\"AP: {ap}\")\n",
    "\n",
    "for i, c in enumerate(predictions.argmax(-1)):\n",
    "    probs = np.array([1 + dict_tags[c].get(t, 0) for t in np.arange(predictions.shape[1])])\n",
    "    probs[c] = 2\n",
    "    predictions[i] = predictions[i] * probs\n",
    "    predictions[i] /= predictions[i].sum()\n",
    "ap = sklearn.metrics.average_precision_score(targets, predictions)\n",
    "\n",
    "print(f\"POST AP: {ap}\")\n",
    "\n",
    "for i, c in enumerate(predictions.argmax(-1)):\n",
    "    probs = np.array([1 + dict_tags[c].get(t, 0) for t in np.arange(predictions.shape[1])])\n",
    "    probs[c] = 2\n",
    "    predictions[i] = predictions[i] * probs\n",
    "    predictions[i] /= predictions[i].sum()\n",
    "ap = sklearn.metrics.average_precision_score(targets, predictions)\n",
    "\n",
    "print(f\"POST-1 AP: {ap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08bb75e9-6cba-4cf1-ba67-2eecc957c3c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "from transformers import set_seed\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "\n",
    "model = Network()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "epochs = 68\n",
    "model = model.to(CUDA_DEV)\n",
    "\n",
    "model.load_state_dict(torch.load(\"./workdir/12_6_0.6_64_64_1_68_1e-05_1e-05_10_1e-07_42/models_full_59.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4daf63c5-45b4-4cb8-99f1-534fc8203caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_idxs, predictions = predict(model, test_dataloader)\n",
    "\n",
    "for i, c in enumerate(predictions.argmax(-1)):\n",
    "    probs = np.array([1 + dict_tags[c].get(t, 0) for t in np.arange(predictions.shape[1])])\n",
    "    probs[c] = 2\n",
    "    predictions[i] = predictions[i] * probs\n",
    "    predictions[i] /= predictions[i].sum()\n",
    "    \n",
    "predictions_df = pd.DataFrame([\n",
    "    {'track': track, 'prediction': ','.join([str(p) for p in probs])}\n",
    "    for track, probs in zip(track_idxs, predictions)\n",
    "])\n",
    "predictions_df.to_csv(f'./workdir/12_6_0.6_64_64_1_68_1e-05_1e-05_10_1e-07_42/prediction_59.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f036bcd-3a07-4a65-b611-bd947b0032fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ya",
   "language": "python",
   "name": "ya"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
